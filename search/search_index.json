{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"llm-strategy \u00b6 Implementing the Strategy Pattern using LLMs. Also, please see https://blog.blackhc.net/2022/12/llm_software_engineering/ for a wider perspective on why this could be important in the future. This package adds a decorator llm_strategy that connects to an LLM (such as OpenAI\u2019s GPT-3) and uses the LLM to \"implement\" abstract methods in interface classes. It does this by forwarding requests to the LLM and converting the responses back to Python data using Python's @dataclasses . It uses the doc strings, type annotations, and method/function names as prompts for the LLM, and can automatically convert the results back into Python types (currently only supporting @dataclasses ). It can also extract a data schema to send to the LLM for interpretation. While the llm-strategy package still relies on some Python code, it has the potential to reduce the need for this code in the future by using additional, cheaper LLMs to automate the parsing of structured data. Github repository : https://github.com/blackhc/llm-strategy/ Documentation https://blackhc.github.io/llm-strategy/ Example \u00b6 from dataclasses import dataclass from llm_strategy import llm_strategy from langchain.llms import OpenAI @llm_strategy(OpenAI(max_tokens=256)) @dataclass class Customer: key: str first_name: str last_name: str birthdate: str address: str @property def age(self) -> int: \"\"\"Return the current age of the customer. This is a computed property based on `birthdate` and the current year (2022). \"\"\" raise NotImplementedError() @dataclass class CustomerDatabase: customers: list[Customer] def find_customer_key(self, query: str) -> list[str]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError() def load(self): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError() def store(self): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError() @llm_strategy(OpenAI(max_tokens=1024)) @dataclass class MockCustomerDatabase(CustomerDatabase): def load(self): self.customers = self.create_mock_customers(10) def store(self): pass @staticmethod def create_mock_customers(num_customers: int = 1) -> list[Customer]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError() See examples/customer_database_search.py for a full example. Getting started with contributing \u00b6 Clone the repository first. Then, install the environment and the pre-commit hooks with make install The CI/CD pipeline will be triggered when you open a pull request, merge to main, or when you create a new release. To finalize the set-up for publishing to PyPi or Artifactory, see here . For activating the automatic documentation with MkDocs, see here . To enable the code coverage reports, see here . Releasing a new version \u00b6 Create an API Token on Pypi . Add the API Token to your projects secrets with the name PYPI_TOKEN by visiting this page . Create a new release on Github. Create a new tag in the form *.*.* . For more details, see here . Repository initiated with fpgmaas/cookiecutter-poetry .","title":"Home"},{"location":"#llm-strategy","text":"Implementing the Strategy Pattern using LLMs. Also, please see https://blog.blackhc.net/2022/12/llm_software_engineering/ for a wider perspective on why this could be important in the future. This package adds a decorator llm_strategy that connects to an LLM (such as OpenAI\u2019s GPT-3) and uses the LLM to \"implement\" abstract methods in interface classes. It does this by forwarding requests to the LLM and converting the responses back to Python data using Python's @dataclasses . It uses the doc strings, type annotations, and method/function names as prompts for the LLM, and can automatically convert the results back into Python types (currently only supporting @dataclasses ). It can also extract a data schema to send to the LLM for interpretation. While the llm-strategy package still relies on some Python code, it has the potential to reduce the need for this code in the future by using additional, cheaper LLMs to automate the parsing of structured data. Github repository : https://github.com/blackhc/llm-strategy/ Documentation https://blackhc.github.io/llm-strategy/","title":"llm-strategy"},{"location":"#example","text":"from dataclasses import dataclass from llm_strategy import llm_strategy from langchain.llms import OpenAI @llm_strategy(OpenAI(max_tokens=256)) @dataclass class Customer: key: str first_name: str last_name: str birthdate: str address: str @property def age(self) -> int: \"\"\"Return the current age of the customer. This is a computed property based on `birthdate` and the current year (2022). \"\"\" raise NotImplementedError() @dataclass class CustomerDatabase: customers: list[Customer] def find_customer_key(self, query: str) -> list[str]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError() def load(self): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError() def store(self): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError() @llm_strategy(OpenAI(max_tokens=1024)) @dataclass class MockCustomerDatabase(CustomerDatabase): def load(self): self.customers = self.create_mock_customers(10) def store(self): pass @staticmethod def create_mock_customers(num_customers: int = 1) -> list[Customer]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError() See examples/customer_database_search.py for a full example.","title":"Example"},{"location":"#getting-started-with-contributing","text":"Clone the repository first. Then, install the environment and the pre-commit hooks with make install The CI/CD pipeline will be triggered when you open a pull request, merge to main, or when you create a new release. To finalize the set-up for publishing to PyPi or Artifactory, see here . For activating the automatic documentation with MkDocs, see here . To enable the code coverage reports, see here .","title":"Getting started with contributing"},{"location":"#releasing-a-new-version","text":"Create an API Token on Pypi . Add the API Token to your projects secrets with the name PYPI_TOKEN by visiting this page . Create a new release on Github. Create a new tag in the form *.*.* . For more details, see here . Repository initiated with fpgmaas/cookiecutter-poetry .","title":"Releasing a new version"},{"location":"reference/SUMMARY/","text":"examples customer_database_search llm_strategy adapters chat_chain llm_function llm_strategy testing fake_chat_model fake_llm","title":"SUMMARY"},{"location":"reference/examples/customer_database_search/","text":"A simple CUI application to visualize and query a customer database using the textual package. Customer dataclass \u00b6 Source code in examples/customer_database_search.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @llm_strategy ( base_llm ) @dataclass class Customer : key : str first_name : str last_name : str birthdate : str address : str @property def age ( self : \"Customer\" ) -> int : \"\"\"Return the current age of the customer. This is a computed property based on `birthdate` and the current year (2022). \"\"\" raise NotImplementedError () age : int property \u00b6 Return the current age of the customer. This is a computed property based on birthdate and the current year (2022). CustomerDatabase dataclass \u00b6 Source code in examples/customer_database_search.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class CustomerDatabase : customers : list [ Customer ] def find_customer_key ( self : \"CustomerDatabase\" , query : str ) -> list [ str ]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError () def load ( self : \"CustomerDatabase\" ): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError () def store ( self : \"CustomerDatabase\" ): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError () find_customer_key ( query ) \u00b6 Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Parameters: Name Type Description Default query str Natural language query required Returns: Type Description list [ str ] The index of the best matching customer in the database. Source code in examples/customer_database_search.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def find_customer_key ( self : \"CustomerDatabase\" , query : str ) -> list [ str ]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError () load () \u00b6 Load the customer database from a file. Source code in examples/customer_database_search.py 56 57 58 def load ( self : \"CustomerDatabase\" ): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError () store () \u00b6 Store the customer database to a file. Source code in examples/customer_database_search.py 60 61 62 def store ( self : \"CustomerDatabase\" ): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError () CustomerDatabaseApp \u00b6 Bases: App A simple textual application to visualize and query a customer database. We show all the customers in a table and allow the user to query the database using natural language in a search box at the bottom of the screen. Source code in examples/customer_database_search.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class CustomerDatabaseApp ( App ): \"\"\"A simple textual application to visualize and query a customer database. We show all the customers in a table and allow the user to query the database using natural language in a search box at the bottom of the screen. \"\"\" PRIORITY_BINDINGS = False BINDINGS = [( \"q\" , \"quit\" , \"Quit the application\" ), ( \"s\" , \"screenshot\" , \"Take a screenshot\" )] database : CustomerDatabase = MockCustomerDatabase ([]) data_table = DataTable ( id = \"customer_table\" ) search_box = Input ( id = \"search_box\" , placeholder = \"Search for a customer (use any kind of query\" ) footer_bar = Horizontal ( search_box ) def on_mount ( self ) -> None : self . database . load () self . data_table . add_columns ( \"First Name\" , \"Last Name\" , \"Birthdate\" , \"Address\" , \"Age\" ) self . search ( \"\" ) def compose ( self ) -> ComposeResult : self . footer_bar . styles . dock = \"bottom\" self . footer_bar . styles . width = \"100%\" self . footer_bar . styles . height = 4 self . data_table . styles . height = \"auto\" self . data_table . styles . width = \"100%\" self . screen . styles . height = \"100%\" self . search_box . styles . width = \"100%\" yield Header () yield self . footer_bar yield Footer () yield self . data_table def search ( self , query : str ): \"\"\"Search the customer database using a natural language query.\"\"\" self . data_table . clear () if not query : for customer in self . database . customers : self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) else : keys = self . database . find_customer_key ( query ) for key in keys : customers_for_key = [ customer for customer in self . database . customers if customer . key == key ] assert len ( customers_for_key ) == 1 customer = customers_for_key [ 0 ] self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) def on_button_pressed ( self , event : Button . Pressed ) -> None : if event . button is self . exit_button : self . exit () def on_input_submitted ( self , event : Input . Submitted ) -> None : if event . input is self . search_box : self . search ( event . value ) search ( query ) \u00b6 Search the customer database using a natural language query. Source code in examples/customer_database_search.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def search ( self , query : str ): \"\"\"Search the customer database using a natural language query.\"\"\" self . data_table . clear () if not query : for customer in self . database . customers : self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) else : keys = self . database . find_customer_key ( query ) for key in keys : customers_for_key = [ customer for customer in self . database . customers if customer . key == key ] assert len ( customers_for_key ) == 1 customer = customers_for_key [ 0 ] self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) MockCustomerDatabase dataclass \u00b6 Bases: CustomerDatabase Source code in examples/customer_database_search.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @llm_strategy ( base_llm ) @dataclass class MockCustomerDatabase ( CustomerDatabase ): def load ( self ): self . customers = self . create_mock_customers ( 10 ) def store ( self ): pass @staticmethod def create_mock_customers ( num_customers : int = 1 ) -> list [ Customer ]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError () create_mock_customers ( num_customers = 1 ) staticmethod \u00b6 Create mock customers with believable data (our customers are world citizens). Source code in examples/customer_database_search.py 74 75 76 77 78 79 @staticmethod def create_mock_customers ( num_customers : int = 1 ) -> list [ Customer ]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError ()","title":"customer_database_search"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.Customer","text":"Source code in examples/customer_database_search.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @llm_strategy ( base_llm ) @dataclass class Customer : key : str first_name : str last_name : str birthdate : str address : str @property def age ( self : \"Customer\" ) -> int : \"\"\"Return the current age of the customer. This is a computed property based on `birthdate` and the current year (2022). \"\"\" raise NotImplementedError ()","title":"Customer"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.Customer.age","text":"Return the current age of the customer. This is a computed property based on birthdate and the current year (2022).","title":"age"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabase","text":"Source code in examples/customer_database_search.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class CustomerDatabase : customers : list [ Customer ] def find_customer_key ( self : \"CustomerDatabase\" , query : str ) -> list [ str ]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError () def load ( self : \"CustomerDatabase\" ): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError () def store ( self : \"CustomerDatabase\" ): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError ()","title":"CustomerDatabase"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabase.find_customer_key","text":"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Parameters: Name Type Description Default query str Natural language query required Returns: Type Description list [ str ] The index of the best matching customer in the database. Source code in examples/customer_database_search.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def find_customer_key ( self : \"CustomerDatabase\" , query : str ) -> list [ str ]: \"\"\"Find the keys of the customers that match a natural language query best (sorted by closeness to the match). We support semantic queries instead of SQL, so we can search for things like \"the customer that was born in 1990\". Args: query: Natural language query Returns: The index of the best matching customer in the database. \"\"\" raise NotImplementedError ()","title":"find_customer_key()"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabase.load","text":"Load the customer database from a file. Source code in examples/customer_database_search.py 56 57 58 def load ( self : \"CustomerDatabase\" ): \"\"\"Load the customer database from a file.\"\"\" raise NotImplementedError ()","title":"load()"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabase.store","text":"Store the customer database to a file. Source code in examples/customer_database_search.py 60 61 62 def store ( self : \"CustomerDatabase\" ): \"\"\"Store the customer database to a file.\"\"\" raise NotImplementedError ()","title":"store()"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabaseApp","text":"Bases: App A simple textual application to visualize and query a customer database. We show all the customers in a table and allow the user to query the database using natural language in a search box at the bottom of the screen. Source code in examples/customer_database_search.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class CustomerDatabaseApp ( App ): \"\"\"A simple textual application to visualize and query a customer database. We show all the customers in a table and allow the user to query the database using natural language in a search box at the bottom of the screen. \"\"\" PRIORITY_BINDINGS = False BINDINGS = [( \"q\" , \"quit\" , \"Quit the application\" ), ( \"s\" , \"screenshot\" , \"Take a screenshot\" )] database : CustomerDatabase = MockCustomerDatabase ([]) data_table = DataTable ( id = \"customer_table\" ) search_box = Input ( id = \"search_box\" , placeholder = \"Search for a customer (use any kind of query\" ) footer_bar = Horizontal ( search_box ) def on_mount ( self ) -> None : self . database . load () self . data_table . add_columns ( \"First Name\" , \"Last Name\" , \"Birthdate\" , \"Address\" , \"Age\" ) self . search ( \"\" ) def compose ( self ) -> ComposeResult : self . footer_bar . styles . dock = \"bottom\" self . footer_bar . styles . width = \"100%\" self . footer_bar . styles . height = 4 self . data_table . styles . height = \"auto\" self . data_table . styles . width = \"100%\" self . screen . styles . height = \"100%\" self . search_box . styles . width = \"100%\" yield Header () yield self . footer_bar yield Footer () yield self . data_table def search ( self , query : str ): \"\"\"Search the customer database using a natural language query.\"\"\" self . data_table . clear () if not query : for customer in self . database . customers : self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) else : keys = self . database . find_customer_key ( query ) for key in keys : customers_for_key = [ customer for customer in self . database . customers if customer . key == key ] assert len ( customers_for_key ) == 1 customer = customers_for_key [ 0 ] self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) def on_button_pressed ( self , event : Button . Pressed ) -> None : if event . button is self . exit_button : self . exit () def on_input_submitted ( self , event : Input . Submitted ) -> None : if event . input is self . search_box : self . search ( event . value )","title":"CustomerDatabaseApp"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.CustomerDatabaseApp.search","text":"Search the customer database using a natural language query. Source code in examples/customer_database_search.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def search ( self , query : str ): \"\"\"Search the customer database using a natural language query.\"\"\" self . data_table . clear () if not query : for customer in self . database . customers : self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), ) else : keys = self . database . find_customer_key ( query ) for key in keys : customers_for_key = [ customer for customer in self . database . customers if customer . key == key ] assert len ( customers_for_key ) == 1 customer = customers_for_key [ 0 ] self . data_table . add_row ( # customer.key, customer . first_name , customer . last_name , customer . birthdate , customer . address , str ( customer . age ), )","title":"search()"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.MockCustomerDatabase","text":"Bases: CustomerDatabase Source code in examples/customer_database_search.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 @llm_strategy ( base_llm ) @dataclass class MockCustomerDatabase ( CustomerDatabase ): def load ( self ): self . customers = self . create_mock_customers ( 10 ) def store ( self ): pass @staticmethod def create_mock_customers ( num_customers : int = 1 ) -> list [ Customer ]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError ()","title":"MockCustomerDatabase"},{"location":"reference/examples/customer_database_search/#examples.customer_database_search.MockCustomerDatabase.create_mock_customers","text":"Create mock customers with believable data (our customers are world citizens). Source code in examples/customer_database_search.py 74 75 76 77 78 79 @staticmethod def create_mock_customers ( num_customers : int = 1 ) -> list [ Customer ]: \"\"\" Create mock customers with believable data (our customers are world citizens). \"\"\" raise NotImplementedError ()","title":"create_mock_customers()"},{"location":"reference/llm_strategy/","text":"","title":"llm_strategy"},{"location":"reference/llm_strategy/adapters/","text":"","title":"adapters"},{"location":"reference/llm_strategy/chat_chain/","text":"ChatChain dataclass \u00b6 Source code in llm_strategy/chat_chain.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @dataclass class ChatChain : chat_model : BaseChatModel messages : list [ BaseMessage ] @property def response ( self ) -> str : assert len ( self . messages ) >= 1 return cast ( str , self . messages [ - 1 ] . content ) def append ( self , messages : list [ BaseMessage ]) -> \"ChatChain\" : return dataclasses . replace ( self , messages = self . messages + messages ) def __add__ ( self , other : list [ BaseMessage ]) -> \"ChatChain\" : return self . append ( other ) def query ( self , question : str , model_args : dict | None = None ) -> Tuple [ str , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: messages = self . messages + [ HumanMessage ( content = question )] model_args = model_args or {} reply = self . chat_model . invoke ( messages , ** model_args ) messages . append ( reply ) return cast ( str , reply . content ), dataclasses . replace ( self , messages = messages ) def enforce_json_response ( self , model_args : dict | None = None ) -> dict : model_args = model_args or {} # Check if the language model is of type \"openai\" and extend model args with a response format in that case model_dict = self . chat_model . dict () if \"openai\" in model_dict [ \"_type\" ] and model_dict . get ( \"model_name\" ) in ( \"gpt-4-1106-preview\" , \"gpt-3.5-turbo-1106\" , ): model_args = { ** model_args , \"response_format\" : dict ( type = \"json_object\" )} return model_args def structured_query ( self , question : str , return_type : type [ B ], model_args : dict | None = None ) -> Tuple [ B , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: if typing . get_origin ( return_type ) is typing . Annotated : return_info = typing . get_args ( return_type ) else : return_info = ( return_type , ... ) output_model = create_model ( \"StructuredOutput\" , result = return_info ) parser : PydanticOutputParser = PydanticOutputParser ( pydantic_object = output_model ) question_and_formatting = question + \" \\n\\n \" + parser . get_format_instructions () reply_content , chain = self . query ( question_and_formatting , ** self . enforce_json_response ( model_args )) parsed_reply : B = typing . cast ( B , parser . parse ( reply_content )) return parsed_reply , chain def branch ( self ) -> \"ChatChain\" : return dataclasses . replace ( self , messages = self . messages . copy ()) query ( question , model_args = None ) \u00b6 Asks a question and returns the result in a single block. Source code in llm_strategy/chat_chain.py 31 32 33 34 35 36 37 38 def query ( self , question : str , model_args : dict | None = None ) -> Tuple [ str , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: messages = self . messages + [ HumanMessage ( content = question )] model_args = model_args or {} reply = self . chat_model . invoke ( messages , ** model_args ) messages . append ( reply ) return cast ( str , reply . content ), dataclasses . replace ( self , messages = messages ) structured_query ( question , return_type , model_args = None ) \u00b6 Asks a question and returns the result in a single block. Source code in llm_strategy/chat_chain.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def structured_query ( self , question : str , return_type : type [ B ], model_args : dict | None = None ) -> Tuple [ B , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: if typing . get_origin ( return_type ) is typing . Annotated : return_info = typing . get_args ( return_type ) else : return_info = ( return_type , ... ) output_model = create_model ( \"StructuredOutput\" , result = return_info ) parser : PydanticOutputParser = PydanticOutputParser ( pydantic_object = output_model ) question_and_formatting = question + \" \\n\\n \" + parser . get_format_instructions () reply_content , chain = self . query ( question_and_formatting , ** self . enforce_json_response ( model_args )) parsed_reply : B = typing . cast ( B , parser . parse ( reply_content )) return parsed_reply , chain","title":"chat_chain"},{"location":"reference/llm_strategy/chat_chain/#llm_strategy.chat_chain.ChatChain","text":"Source code in llm_strategy/chat_chain.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @dataclass class ChatChain : chat_model : BaseChatModel messages : list [ BaseMessage ] @property def response ( self ) -> str : assert len ( self . messages ) >= 1 return cast ( str , self . messages [ - 1 ] . content ) def append ( self , messages : list [ BaseMessage ]) -> \"ChatChain\" : return dataclasses . replace ( self , messages = self . messages + messages ) def __add__ ( self , other : list [ BaseMessage ]) -> \"ChatChain\" : return self . append ( other ) def query ( self , question : str , model_args : dict | None = None ) -> Tuple [ str , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: messages = self . messages + [ HumanMessage ( content = question )] model_args = model_args or {} reply = self . chat_model . invoke ( messages , ** model_args ) messages . append ( reply ) return cast ( str , reply . content ), dataclasses . replace ( self , messages = messages ) def enforce_json_response ( self , model_args : dict | None = None ) -> dict : model_args = model_args or {} # Check if the language model is of type \"openai\" and extend model args with a response format in that case model_dict = self . chat_model . dict () if \"openai\" in model_dict [ \"_type\" ] and model_dict . get ( \"model_name\" ) in ( \"gpt-4-1106-preview\" , \"gpt-3.5-turbo-1106\" , ): model_args = { ** model_args , \"response_format\" : dict ( type = \"json_object\" )} return model_args def structured_query ( self , question : str , return_type : type [ B ], model_args : dict | None = None ) -> Tuple [ B , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: if typing . get_origin ( return_type ) is typing . Annotated : return_info = typing . get_args ( return_type ) else : return_info = ( return_type , ... ) output_model = create_model ( \"StructuredOutput\" , result = return_info ) parser : PydanticOutputParser = PydanticOutputParser ( pydantic_object = output_model ) question_and_formatting = question + \" \\n\\n \" + parser . get_format_instructions () reply_content , chain = self . query ( question_and_formatting , ** self . enforce_json_response ( model_args )) parsed_reply : B = typing . cast ( B , parser . parse ( reply_content )) return parsed_reply , chain def branch ( self ) -> \"ChatChain\" : return dataclasses . replace ( self , messages = self . messages . copy ())","title":"ChatChain"},{"location":"reference/llm_strategy/chat_chain/#llm_strategy.chat_chain.ChatChain.query","text":"Asks a question and returns the result in a single block. Source code in llm_strategy/chat_chain.py 31 32 33 34 35 36 37 38 def query ( self , question : str , model_args : dict | None = None ) -> Tuple [ str , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: messages = self . messages + [ HumanMessage ( content = question )] model_args = model_args or {} reply = self . chat_model . invoke ( messages , ** model_args ) messages . append ( reply ) return cast ( str , reply . content ), dataclasses . replace ( self , messages = messages )","title":"query()"},{"location":"reference/llm_strategy/chat_chain/#llm_strategy.chat_chain.ChatChain.structured_query","text":"Asks a question and returns the result in a single block. Source code in llm_strategy/chat_chain.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def structured_query ( self , question : str , return_type : type [ B ], model_args : dict | None = None ) -> Tuple [ B , \"ChatChain\" ]: \"\"\"Asks a question and returns the result in a single block.\"\"\" # Build messages: if typing . get_origin ( return_type ) is typing . Annotated : return_info = typing . get_args ( return_type ) else : return_info = ( return_type , ... ) output_model = create_model ( \"StructuredOutput\" , result = return_info ) parser : PydanticOutputParser = PydanticOutputParser ( pydantic_object = output_model ) question_and_formatting = question + \" \\n\\n \" + parser . get_format_instructions () reply_content , chain = self . query ( question_and_formatting , ** self . enforce_json_response ( model_args )) parsed_reply : B = typing . cast ( B , parser . parse ( reply_content )) return parsed_reply , chain","title":"structured_query()"},{"location":"reference/llm_strategy/llm_function/","text":"LLMBoundSignature dataclass \u00b6 A function call that can be used to generate a prompt. Source code in llm_strategy/llm_function.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @dataclass class LLMBoundSignature : \"\"\" A function call that can be used to generate a prompt. \"\"\" structured_prompt : LLMStructuredPrompt signature : inspect . Signature @property def input_type ( self ) -> type [ BaseModel ]: \"\"\"Return the input type.\"\"\" return self . structured_prompt . input_type @property def output_type ( self ) -> type [ BaseModel ]: \"\"\"Return the output type.\"\"\" return self . structured_prompt . output_type @property def docstring ( self ) -> str : \"\"\"Return the docstring.\"\"\" return self . structured_prompt . docstring @property def return_annotation ( self ) -> str : \"\"\"Return the name.\"\"\" return self . structured_prompt . return_annotation def get_input_object ( self , * args : P . args , ** kwargs : P . kwargs ) -> BaseModel : \"\"\"Call the function and return the inputs.\"\"\" # bind the inputs to the signature bound_arguments = LLMBoundSignature . bind ( self . signature , args , kwargs ) # get the arguments arguments = bound_arguments . arguments inputs = self . structured_prompt . input_type ( ** arguments ) return inputs @staticmethod def from_call ( f : typing . Callable [ P , T ], args : P . args , kwargs : P . kwargs ) -> \"LLMBoundSignature\" : # noqa: C901 \"\"\"Create an LLMBoundSignature from a function. Args: f: The function to create the LLMBoundSignature from. args: The positional arguments to the function (but excluding the language model/first param). kwargs: The keyword arguments to the function. \"\"\" # get clean docstring docstring = inspect . getdoc ( f ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( f , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) return_type = signature . return_annotation if return_type is inspect . Parameter . empty : raise ValueError ( \"The function must have a return type.\" ) # create a pydantic model from the parameters parameter_dict = LLMBoundSignature . parameter_items_to_field_tuple ( parameters_items [ 1 :]) # turn function name into a class name class_name = string . capwords ( f . __name__ , sep = \"_\" ) . replace ( \"_\" , \"\" ) # create the input model # collect all __parameters__ from the type annotations # this is necessary because we need to know the types of the parameters # to create the pydantic model generic_parameters : set [ typing . TypeVar ] = set () for parameter in parameter_dict . values (): annotation = parameter [ 0 ] # unwrap any Annotated types while hasattr ( annotation , \"__metadata__\" ): annotation = annotation . __origin__ # if the annotation is already a type variable, add it to the set if isinstance ( annotation , typing . TypeVar ): generic_parameters . add ( annotation ) # if the annotation is a generic type, add the parameters to the set if hasattr ( annotation , \"__parameters__\" ): generic_parameters . update ( annotation . __parameters__ ) model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) if generic_parameters : bases = ( pydantic . generics . GenericModel , typing . Generic [ * generic_parameters ]) input_type = create_model ( f \" { class_name } Inputs\" , __base__ = bases , __module__ = f . __module__ , ** model_spec ) else : input_type = create_model ( f \" { class_name } Inputs\" , __module__ = f . __module__ , ** model_spec ) input_type . update_forward_refs () # update parameter_dict types with bound_arguments # this ensures that we serialize the actual types # might not be optimal because the language model won't be aware of original types, however bound_arguments = LLMBoundSignature . bind ( signature , args , kwargs ) for parameter_name in parameter_dict : if parameter_name in bound_arguments . arguments : parameter_dict [ parameter_name ] = ( type ( bound_arguments . arguments [ parameter_name ]), parameter_dict [ parameter_name ][ 1 ], ) specific_model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) specific_input_type = create_model ( f \"Specific { class_name } Inputs\" , __module__ = f . __module__ , ** specific_model_spec ) specific_input_type . update_forward_refs () input = specific_input_type ( ** bound_arguments . arguments ) llm_structured_prompt : LLMStructuredPrompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = input_type , return_annotation = return_type , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) @staticmethod def parameter_items_to_field_tuple ( parameters_items : list [ tuple [ str , inspect . Parameter ]]): \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , parameter in parameters_items : # every parameter must be annotated or have a default value annotation = parameter . annotation if annotation is type : annotation = TyperWrapper if parameter . default is inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , ... ) else : parameter_dict [ parameter_name ] = ( annotation , parameter . default ) return parameter_dict @staticmethod def field_tuples_to_model_spec ( field_tuples_dict : dict [ str , tuple [ str , tuple [ type , ... ]]] ) -> dict [ str , tuple [ type , object ] | object ]: \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , ( annotation , default ) in field_tuples_dict . items (): # every parameter must be annotated or have a default value if default is ... : parameter_dict [ parameter_name ] = ( annotation , ... ) else : if annotation is not inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , default ) else : parameter_dict [ parameter_name ] = default return parameter_dict @staticmethod def get_or_create_pydantic_default ( field : FieldInfo ): if field . default is not Undefined : if field . default is Ellipsis : return inspect . Parameter . empty return field . default if field . default_factory is not None : return field . default_factory () return None @staticmethod def bind ( signature , args , kwargs ): \"\"\" Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. \"\"\" # resolve parameter defaults to FieldInfo.default if the parameter is a field signature_fixed_defaults = signature . replace ( parameters = [ parameter . replace ( default = LLMBoundSignature . get_or_create_pydantic_default ( parameter . default )) if isinstance ( parameter . default , FieldInfo ) else parameter for parameter in list ( signature . parameters . values ())[ 1 :] ] ) bound_arguments = signature_fixed_defaults . bind ( * args , ** kwargs ) bound_arguments . apply_defaults () return bound_arguments docstring : str property \u00b6 Return the docstring. input_type : type [ BaseModel ] property \u00b6 Return the input type. output_type : type [ BaseModel ] property \u00b6 Return the output type. return_annotation : str property \u00b6 Return the name. bind ( signature , args , kwargs ) staticmethod \u00b6 Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. Source code in llm_strategy/llm_function.py 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @staticmethod def bind ( signature , args , kwargs ): \"\"\" Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. \"\"\" # resolve parameter defaults to FieldInfo.default if the parameter is a field signature_fixed_defaults = signature . replace ( parameters = [ parameter . replace ( default = LLMBoundSignature . get_or_create_pydantic_default ( parameter . default )) if isinstance ( parameter . default , FieldInfo ) else parameter for parameter in list ( signature . parameters . values ())[ 1 :] ] ) bound_arguments = signature_fixed_defaults . bind ( * args , ** kwargs ) bound_arguments . apply_defaults () return bound_arguments field_tuples_to_model_spec ( field_tuples_dict ) staticmethod \u00b6 Get the parameter definitions for a function call from the parameters and arguments. Source code in llm_strategy/llm_function.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 @staticmethod def field_tuples_to_model_spec ( field_tuples_dict : dict [ str , tuple [ str , tuple [ type , ... ]]] ) -> dict [ str , tuple [ type , object ] | object ]: \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , ( annotation , default ) in field_tuples_dict . items (): # every parameter must be annotated or have a default value if default is ... : parameter_dict [ parameter_name ] = ( annotation , ... ) else : if annotation is not inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , default ) else : parameter_dict [ parameter_name ] = default return parameter_dict from_call ( f , args , kwargs ) staticmethod \u00b6 Create an LLMBoundSignature from a function. Parameters: Name Type Description Default f Callable [ P , T ] The function to create the LLMBoundSignature from. required args args The positional arguments to the function (but excluding the language model/first param). required kwargs kwargs The keyword arguments to the function. required Source code in llm_strategy/llm_function.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 @staticmethod def from_call ( f : typing . Callable [ P , T ], args : P . args , kwargs : P . kwargs ) -> \"LLMBoundSignature\" : # noqa: C901 \"\"\"Create an LLMBoundSignature from a function. Args: f: The function to create the LLMBoundSignature from. args: The positional arguments to the function (but excluding the language model/first param). kwargs: The keyword arguments to the function. \"\"\" # get clean docstring docstring = inspect . getdoc ( f ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( f , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) return_type = signature . return_annotation if return_type is inspect . Parameter . empty : raise ValueError ( \"The function must have a return type.\" ) # create a pydantic model from the parameters parameter_dict = LLMBoundSignature . parameter_items_to_field_tuple ( parameters_items [ 1 :]) # turn function name into a class name class_name = string . capwords ( f . __name__ , sep = \"_\" ) . replace ( \"_\" , \"\" ) # create the input model # collect all __parameters__ from the type annotations # this is necessary because we need to know the types of the parameters # to create the pydantic model generic_parameters : set [ typing . TypeVar ] = set () for parameter in parameter_dict . values (): annotation = parameter [ 0 ] # unwrap any Annotated types while hasattr ( annotation , \"__metadata__\" ): annotation = annotation . __origin__ # if the annotation is already a type variable, add it to the set if isinstance ( annotation , typing . TypeVar ): generic_parameters . add ( annotation ) # if the annotation is a generic type, add the parameters to the set if hasattr ( annotation , \"__parameters__\" ): generic_parameters . update ( annotation . __parameters__ ) model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) if generic_parameters : bases = ( pydantic . generics . GenericModel , typing . Generic [ * generic_parameters ]) input_type = create_model ( f \" { class_name } Inputs\" , __base__ = bases , __module__ = f . __module__ , ** model_spec ) else : input_type = create_model ( f \" { class_name } Inputs\" , __module__ = f . __module__ , ** model_spec ) input_type . update_forward_refs () # update parameter_dict types with bound_arguments # this ensures that we serialize the actual types # might not be optimal because the language model won't be aware of original types, however bound_arguments = LLMBoundSignature . bind ( signature , args , kwargs ) for parameter_name in parameter_dict : if parameter_name in bound_arguments . arguments : parameter_dict [ parameter_name ] = ( type ( bound_arguments . arguments [ parameter_name ]), parameter_dict [ parameter_name ][ 1 ], ) specific_model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) specific_input_type = create_model ( f \"Specific { class_name } Inputs\" , __module__ = f . __module__ , ** specific_model_spec ) specific_input_type . update_forward_refs () input = specific_input_type ( ** bound_arguments . arguments ) llm_structured_prompt : LLMStructuredPrompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = input_type , return_annotation = return_type , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) get_input_object ( * args , ** kwargs ) \u00b6 Call the function and return the inputs. Source code in llm_strategy/llm_function.py 517 518 519 520 521 522 523 524 def get_input_object ( self , * args : P . args , ** kwargs : P . kwargs ) -> BaseModel : \"\"\"Call the function and return the inputs.\"\"\" # bind the inputs to the signature bound_arguments = LLMBoundSignature . bind ( self . signature , args , kwargs ) # get the arguments arguments = bound_arguments . arguments inputs = self . structured_prompt . input_type ( ** arguments ) return inputs parameter_items_to_field_tuple ( parameters_items ) staticmethod \u00b6 Get the parameter definitions for a function call from the parameters and arguments. Source code in llm_strategy/llm_function.py 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 @staticmethod def parameter_items_to_field_tuple ( parameters_items : list [ tuple [ str , inspect . Parameter ]]): \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , parameter in parameters_items : # every parameter must be annotated or have a default value annotation = parameter . annotation if annotation is type : annotation = TyperWrapper if parameter . default is inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , ... ) else : parameter_dict [ parameter_name ] = ( annotation , parameter . default ) return parameter_dict LLMExplicitFunction \u00b6 Bases: LLMFunctionInterface [ P , T ] , Generic [ P , T ] A callable that can be called with a chat model. Source code in llm_strategy/llm_function.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 class LLMExplicitFunction ( LLMFunctionInterface [ P , T ], typing . Generic [ P , T ]): \"\"\" A callable that can be called with a chat model. \"\"\" def llm_bound_signature ( self , input : BaseModel ) -> LLMBoundSignature : \"\"\"Create an LLMFunctionSpec from a function.\"\"\" # get clean docstring of docstring = inspect . getdoc ( self ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( self , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) second_parameter : inspect . Parameter = parameters_items [ 1 ][ 1 ] llm_structured_prompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = second_parameter . annotation , return_annotation = signature . return_annotation , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) def __getattr__ ( self , item ): return getattr ( self . __wrapped__ , item ) @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = True , capture_args = slicer [ 1 :]) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input : BaseModel ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , input ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = self . llm_bound_signature ( input ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value __call__ ( language_model_or_chat_chain , input ) \u00b6 Call the function. Source code in llm_strategy/llm_function.py 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = True , capture_args = slicer [ 1 :]) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input : BaseModel ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , input ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = self . llm_bound_signature ( input ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value __get__ ( instance , owner = None ) \u00b6 Support instance methods. Source code in llm_strategy/llm_function.py 795 796 797 798 799 800 801 def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) llm_bound_signature ( input ) \u00b6 Create an LLMFunctionSpec from a function. Source code in llm_strategy/llm_function.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 def llm_bound_signature ( self , input : BaseModel ) -> LLMBoundSignature : \"\"\"Create an LLMFunctionSpec from a function.\"\"\" # get clean docstring of docstring = inspect . getdoc ( self ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( self , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) second_parameter : inspect . Parameter = parameters_items [ 1 ][ 1 ] llm_structured_prompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = second_parameter . annotation , return_annotation = signature . return_annotation , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) LLMFunction \u00b6 Bases: LLMFunctionInterface [ P , T ] , Generic [ P , T ] A callable that can be called with a chat model. Source code in llm_strategy/llm_function.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 class LLMFunction ( LLMFunctionInterface [ P , T ], typing . Generic [ P , T ]): \"\"\" A callable that can be called with a chat model. \"\"\" def llm_bound_signature ( self , * args , ** kwargs ) -> LLMBoundSignature : return LLMBoundSignature . from_call ( self , args , kwargs ) def get_input_object ( self , * args , ** kwargs ) -> BaseModel : return self . llm_bound_signature ( * args , ** kwargs ) . get_input_object ( * args , ** kwargs ) def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) def __getattr__ ( self , item ): return getattr ( self . __wrapped__ , item ) def explicit ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input_object : BaseModel ): \"\"\"Call the function with explicit inputs.\"\"\" return self ( language_model_or_chat_chain , ** dict ( input_object )) @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = slicer [ 1 :], capture_args = True ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , * args : P . args , ** kwargs : P . kwargs , ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , * args , ** kwargs ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = LLMBoundSignature . from_call ( self , args , kwargs ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value __call__ ( language_model_or_chat_chain , * args , ** kwargs ) \u00b6 Call the function. Source code in llm_strategy/llm_function.py 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = slicer [ 1 :], capture_args = True ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , * args : P . args , ** kwargs : P . kwargs , ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , * args , ** kwargs ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = LLMBoundSignature . from_call ( self , args , kwargs ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value __get__ ( instance , owner = None ) \u00b6 Support instance methods. Source code in llm_strategy/llm_function.py 714 715 716 717 718 719 720 def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) explicit ( language_model_or_chat_chain , input_object ) \u00b6 Call the function with explicit inputs. Source code in llm_strategy/llm_function.py 725 726 727 728 def explicit ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input_object : BaseModel ): \"\"\"Call the function with explicit inputs.\"\"\" return self ( language_model_or_chat_chain , ** dict ( input_object )) LLMStructuredPrompt dataclass \u00b6 Bases: Generic [ B , T ] A structured prompt for a language model. Source code in llm_strategy/llm_function.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 @dataclass class LLMStructuredPrompt ( typing . Generic [ B , T ]): \"\"\" A structured prompt for a language model. \"\"\" docstring : str input_type : type [ B ] output_type : type [ Output [ T ]] return_annotation : T input : B @staticmethod def extract_from_definitions ( definitions : dict , type_ : type ) -> dict : normalized_name = pydantic . schema . normalize_name ( type_ . __name__ ) sub_schema = definitions [ normalized_name ] del definitions [ normalized_name ] return sub_schema def get_json_schema ( self , exclude_default : bool = True ) -> dict : schema = pydantic . schema . schema ([ self . input_type , self . output_type ], ref_template = \" {model} \" ) definitions : dict = deepcopy ( schema [ \"definitions\" ]) # remove title and type from each sub dict in the definitions for value in definitions . values (): value . pop ( \"title\" ) value . pop ( \"type\" ) for property in value . get ( \"properties\" , {}) . values (): property . pop ( \"title\" , None ) if exclude_default : property . pop ( \"default\" , None ) input_schema = self . extract_from_definitions ( definitions , self . input_type ) output_schema = self . extract_from_definitions ( definitions , self . output_type ) schema = dict ( input_schema = input_schema , output_schema = output_schema , additional_definitions = definitions , ) return schema @staticmethod def create ( docstring : str , input_type : type [ B ], return_annotation : T , input : B ) -> \"LLMStructuredPrompt[B, T]\" : \"\"\"Create an LLMExplicitFunction.\"\"\" # determine the return type # the return type can be a type annotation or an Annotated type with annotation being a FieldInfo if typing . get_origin ( return_annotation ) is typing . Annotated : return_info = typing . get_args ( return_annotation ) else : return_info = ( return_annotation , ... ) # resolve generic types generic_type_map = LLMStructuredPrompt . resolve_generic_types ( input_type , input ) return_type : type = LLMStructuredPrompt . resolve_type ( return_info [ 0 ], generic_type_map ) if return_type is types . NoneType : # noqa: E721 raise ValueError ( f \"Resolve return type { return_info [ 0 ] } is None! This would be a NOP.\" ) return_info = ( return_type , return_info [ 1 ]) if typing . get_origin ( return_annotation ) is typing . Annotated : assert hasattr ( return_annotation , \"copy_with\" ) resolved_return_annotation = return_annotation . copy_with ([ return_info [ 0 ]]) else : resolved_return_annotation = return_info [ 0 ] # create the output model resolved_output_model_type = Output [ return_type ] # noqa # resolve input_type resolved_input_type = LLMStructuredPrompt . resolve_type ( input_type , generic_type_map ) return LLMStructuredPrompt ( docstring = docstring , input_type = resolved_input_type , output_type = resolved_output_model_type , return_annotation = resolved_return_annotation , input = input , ) @staticmethod def resolve_type ( source_type : type , generic_type_map : dict [ type , type ]) -> type : \"\"\" Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. \"\"\" if source_type in generic_type_map : source_type = generic_type_map [ source_type ] if isinstance ( source_type , type ) and issubclass ( source_type , generics . GenericModel ): base_generic_type = LLMStructuredPrompt . get_base_generic_type ( source_type ) generic_parameter_type_map = LLMStructuredPrompt . get_generic_type_map ( source_type , base_generic_type ) # forward step using the generic type map resolved_generic_type_map = { generic_type : generic_type_map . get ( target_type , target_type ) for generic_type , target_type in generic_parameter_type_map . items () } resolved_tuple = tuple ( resolved_generic_type_map [ generic_type ] for generic_type in base_generic_type . __parameters__ ) source_type = base_generic_type [ resolved_tuple ] else : # we let Pydantic handle the rest source_type = replace_types ( source_type , generic_type_map ) return source_type @staticmethod def resolve_generic_types ( model : type [ BaseModel ], instance : BaseModel ): generic_type_map : dict = {} for field_name , attr_value in list ( instance ): if field_name not in model . __annotations__ : continue annotation = model . __annotations__ [ field_name ] # if the annotation is an Annotated type, get the type annotation if typing . get_origin ( annotation ) is typing . Annotated : annotation = typing . get_args ( annotation )[ 0 ] # if the annotation is a type var, resolve it into the generic type map if isinstance ( annotation , typing . TypeVar ): LLMStructuredPrompt . add_resolved_type ( generic_type_map , annotation , type ( attr_value )) # if the annotation is a generic type alias ignore elif isinstance ( annotation , types . GenericAlias ): continue # if the annotation is a type, check if it is a generic type elif issubclass ( annotation , generics . GenericModel ): # check if the type is in generics._assigned_parameters generic_definition_type_map = LLMStructuredPrompt . get_generic_type_map ( annotation ) argument_type = type ( attr_value ) generic_instance_type_map = LLMStructuredPrompt . get_generic_type_map ( argument_type ) assert list ( generic_definition_type_map . keys ()) == list ( generic_instance_type_map . keys ()) # update the generic type map # if the generic type is already in the map, check that it is the same for generic_parameter , generic_parameter_target in generic_definition_type_map . items (): if generic_parameter_target not in annotation . __parameters__ : continue resolved_type = generic_instance_type_map [ generic_parameter ] LLMStructuredPrompt . add_resolved_type ( generic_type_map , generic_parameter_target , resolved_type ) return generic_type_map @staticmethod def add_resolved_type ( generic_type_map , source_type , resolved_type ): \"\"\" Add a resolved type to the generic type map. \"\"\" if source_type in generic_type_map : # TODO: support finding the common base class? if ( previous_resolution := generic_type_map [ source_type ]) is not resolved_type : raise ValueError ( f \"Cannot resolve generic type { source_type } , conflicting \" f \"resolution: { previous_resolution } and { resolved_type } .\" ) else : generic_type_map [ source_type ] = resolved_type @staticmethod def get_generic_type_map ( generic_type , base_generic_type = None ): if base_generic_type is None : base_generic_type = LLMStructuredPrompt . get_base_generic_type ( generic_type ) base_classes = inspect . getmro ( generic_type ) # we have to iterate through the base classes generic_parameter_type_map = { generic_type : generic_type for generic_type in generic_type . __parameters__ } for base_class in base_classes : # skip baseclasses that are from pydantic.generic # this avoids a bug that is caused by generics.GenericModel.__parameterized_bases_ if base_class . __module__ == \"pydantic.generics\" : continue if issubclass ( base_class , base_generic_type ): if base_class in generics . _assigned_parameters : assignment = generics . _assigned_parameters [ base_class ] generic_parameter_type_map = { old_generic_type : generic_parameter_type_map . get ( new_generic_type , new_generic_type ) for old_generic_type , new_generic_type in assignment . items () } return generic_parameter_type_map @staticmethod def get_base_generic_type ( field_type ) -> type [ generics . GenericModel ]: # get the base class name from annotation (which is without []) base_generic_name = field_type . __name__ if \"[\" in field_type . __name__ : base_generic_name = field_type . __name__ . split ( \"[\" )[ 0 ] # get the base class from argument_type_base_classes with base_generic_name for base_class in reversed ( inspect . getmro ( field_type )): if base_class . __name__ == base_generic_name and issubclass ( field_type , base_class ): base_generic_type = base_class break else : raise ValueError ( f \"Could not find base generic type { base_generic_name } for { field_type } .\" ) return base_generic_type @trace_calls ( name = \"LLMStructuredPrompt\" , kind = TraceNodeKind . CHAIN , capture_args = False , capture_return = False ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , ) -> T : \"\"\"Call the function.\"\"\" # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # get the input and output schema as JSON dict schema = self . get_json_schema () # print(json.dumps(schema, indent=1)) update_json_schema_hyperparameters ( schema , Hyperparameter ( \"json_schema\" ) @ get_json_schema_hyperparameters ( schema ), ) update_event_properties ( dict ( arguments = dict ( self . input ), ) ) parsed_output = self . query ( language_model_or_chat_chain , schema ) # print(f\"Input: {self.input.json(indent=1)}\") # print(f\"Output: {json.dumps(json.loads(parsed_output.json())['return_value'], indent=1)}\") update_event_properties ( dict ( result = parsed_output . return_value )) return parsed_output . return_value @track_hyperparameters def query ( self , language_model_or_chat_chain , schema ): # noqa: C901 # create the prompt json_dumps_kwargs = Hyperparameter ( \"json_dumps_kwargs\" ) @ dict ( indent = None ) additional_definitions_prompt_template = Hyperparameter ( \"additional_definitions_prompt_template\" , \"Here is the schema for additional data types: \\n ``` \\n {additional_definitions} \\n ``` \\n\\n \" , ) optional_additional_definitions_prompt = \"\" if schema [ \"additional_definitions\" ]: optional_additional_definitions_prompt = additional_definitions_prompt_template . format ( additional_definitions = json . dumps ( schema [ \"additional_definitions\" ], ** json_dumps_kwargs ) ) prompt = ( Hyperparameter ( \"llm_structured_prompt_template\" , description = ( \"The general-purpose prompt for the structured prompt execution. It tells the LLM what to \" \"do and how to read function arguments and structure return values. \" ), ) @ ' {docstring} \\n\\n The input and output are formatted as a JSON interface that conforms to the JSON schemas below. \\n\\n As an example, for the schema {{\"properties\": {{\"foo\": {{\"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. \\n\\n {optional_additional_definitions_prompt} Here is the input schema: \\n ``` \\n {input_schema} \\n ``` \\n\\n Here is the output schema: \\n ``` \\n {output_schema} \\n ``` \\n Now output the results for the following inputs: \\n ``` \\n {inputs} \\n ```' ) . format ( docstring = self . docstring , optional_additional_definitions_prompt = optional_additional_definitions_prompt , input_schema = json . dumps ( schema [ \"input_schema\" ], ** json_dumps_kwargs ), output_schema = json . dumps ( schema [ \"output_schema\" ], ** json_dumps_kwargs ), inputs = self . input . json ( ** json_dumps_kwargs ), ) # get the response num_retries = Hyperparameter ( \"num_retries_on_parser_failure\" ) @ 3 if language_model_or_chat_chain is None : raise ValueError ( \"The language model or chat chain must be provided.\" ) if isinstance ( language_model_or_chat_chain , BaseChatModel ): language_model_or_chat_chain = ChatChain ( language_model_or_chat_chain , []) if isinstance ( language_model_or_chat_chain , ChatChain ): chain = language_model_or_chat_chain for _ in range ( num_retries ): output , chain = chain . query ( prompt , model_args = chain . enforce_json_response ()) try : parsed_output = parse ( output , self . output_type ) break except OutputParserException as e : prompt = ( Hyperparameter ( \"error_prompt\" ) @ \"Tried to parse your output but failed: \\n\\n \" + str ( e ) + Hyperparameter ( \"retry_prompt\" ) @ \" \\n\\n Please try again and avoid this issue.\" ) else : exception = OutputParserException ( f \"Failed to parse the output after { num_retries } retries.\" ) exception . add_note ( chain ) raise exception elif isinstance ( language_model_or_chat_chain , BaseLLM ): model : BaseChatModel = language_model_or_chat_chain # Check if the language model is of type \"openai\" and extend model args with a response format in that case model_dict = model . dict () if \"openai\" in model_dict [ \"_type\" ] and model_dict . get ( \"model_name\" ) in ( \"gpt-4-1106-preview\" , \"gpt-3.5-turbo-1106\" , ): model_args = dict ( response_format = dict ( type = \"json_object\" )) else : model_args = {} for _ in range ( num_retries ): output = model ( prompt , ** model_args ) try : parsed_output = parse ( output , self . output_type ) break except OutputParserException as e : prompt = ( prompt + Hyperparameter ( \"output_prompt\" ) @ \" \\n\\n Received the output \\n\\n \" + output + Hyperparameter ( \"error_prompt\" ) @ \"Tried to parse your output but failed: \\n\\n \" + str ( e ) + Hyperparameter ( \"retry_prompt\" ) @ \" \\n\\n Please try again and avoid this issue.\" ) else : exception = OutputParserException ( f \"Failed to parse the output after { num_retries } retries.\" ) exception . add_note ( prompt ) raise exception else : raise ValueError ( \"The language model or chat chain must be provided.\" ) return parsed_output __call__ ( language_model_or_chat_chain ) \u00b6 Call the function. Source code in llm_strategy/llm_function.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @trace_calls ( name = \"LLMStructuredPrompt\" , kind = TraceNodeKind . CHAIN , capture_args = False , capture_return = False ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , ) -> T : \"\"\"Call the function.\"\"\" # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # get the input and output schema as JSON dict schema = self . get_json_schema () # print(json.dumps(schema, indent=1)) update_json_schema_hyperparameters ( schema , Hyperparameter ( \"json_schema\" ) @ get_json_schema_hyperparameters ( schema ), ) update_event_properties ( dict ( arguments = dict ( self . input ), ) ) parsed_output = self . query ( language_model_or_chat_chain , schema ) # print(f\"Input: {self.input.json(indent=1)}\") # print(f\"Output: {json.dumps(json.loads(parsed_output.json())['return_value'], indent=1)}\") update_event_properties ( dict ( result = parsed_output . return_value )) return parsed_output . return_value add_resolved_type ( generic_type_map , source_type , resolved_type ) staticmethod \u00b6 Add a resolved type to the generic type map. Source code in llm_strategy/llm_function.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 @staticmethod def add_resolved_type ( generic_type_map , source_type , resolved_type ): \"\"\" Add a resolved type to the generic type map. \"\"\" if source_type in generic_type_map : # TODO: support finding the common base class? if ( previous_resolution := generic_type_map [ source_type ]) is not resolved_type : raise ValueError ( f \"Cannot resolve generic type { source_type } , conflicting \" f \"resolution: { previous_resolution } and { resolved_type } .\" ) else : generic_type_map [ source_type ] = resolved_type create ( docstring , input_type , return_annotation , input ) staticmethod \u00b6 Create an LLMExplicitFunction. Source code in llm_strategy/llm_function.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 @staticmethod def create ( docstring : str , input_type : type [ B ], return_annotation : T , input : B ) -> \"LLMStructuredPrompt[B, T]\" : \"\"\"Create an LLMExplicitFunction.\"\"\" # determine the return type # the return type can be a type annotation or an Annotated type with annotation being a FieldInfo if typing . get_origin ( return_annotation ) is typing . Annotated : return_info = typing . get_args ( return_annotation ) else : return_info = ( return_annotation , ... ) # resolve generic types generic_type_map = LLMStructuredPrompt . resolve_generic_types ( input_type , input ) return_type : type = LLMStructuredPrompt . resolve_type ( return_info [ 0 ], generic_type_map ) if return_type is types . NoneType : # noqa: E721 raise ValueError ( f \"Resolve return type { return_info [ 0 ] } is None! This would be a NOP.\" ) return_info = ( return_type , return_info [ 1 ]) if typing . get_origin ( return_annotation ) is typing . Annotated : assert hasattr ( return_annotation , \"copy_with\" ) resolved_return_annotation = return_annotation . copy_with ([ return_info [ 0 ]]) else : resolved_return_annotation = return_info [ 0 ] # create the output model resolved_output_model_type = Output [ return_type ] # noqa # resolve input_type resolved_input_type = LLMStructuredPrompt . resolve_type ( input_type , generic_type_map ) return LLMStructuredPrompt ( docstring = docstring , input_type = resolved_input_type , output_type = resolved_output_model_type , return_annotation = resolved_return_annotation , input = input , ) resolve_type ( source_type , generic_type_map ) staticmethod \u00b6 Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. Source code in llm_strategy/llm_function.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @staticmethod def resolve_type ( source_type : type , generic_type_map : dict [ type , type ]) -> type : \"\"\" Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. \"\"\" if source_type in generic_type_map : source_type = generic_type_map [ source_type ] if isinstance ( source_type , type ) and issubclass ( source_type , generics . GenericModel ): base_generic_type = LLMStructuredPrompt . get_base_generic_type ( source_type ) generic_parameter_type_map = LLMStructuredPrompt . get_generic_type_map ( source_type , base_generic_type ) # forward step using the generic type map resolved_generic_type_map = { generic_type : generic_type_map . get ( target_type , target_type ) for generic_type , target_type in generic_parameter_type_map . items () } resolved_tuple = tuple ( resolved_generic_type_map [ generic_type ] for generic_type in base_generic_type . __parameters__ ) source_type = base_generic_type [ resolved_tuple ] else : # we let Pydantic handle the rest source_type = replace_types ( source_type , generic_type_map ) return source_type TyperWrapper \u00b6 Bases: str A wrapper around a type that can be used to create a Pydantic model. This is used to support @classmethods. Source code in llm_strategy/llm_function.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class TyperWrapper ( str ): \"\"\" A wrapper around a type that can be used to create a Pydantic model. This is used to support @classmethods. \"\"\" @classmethod def __get_validators__ ( cls ) -> typing . Iterator [ typing . Callable ]: # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def validate ( cls , v : type ) -> str : if not isinstance ( v , type ): raise TypeError ( \"type required\" ) return v . __qualname__ apply_decorator ( f , decorator ) \u00b6 Apply a decorator to a function. This function is used to apply a decorator to a function, while preserving the function type. This is useful when we want to apply a decorator to a function that is a classmethod, staticmethod, property, or a method of a class. Parameters \u00b6 F_types The function to decorate. decorator: Callable The decorator to apply. Returns \u00b6 F_types The decorated function. Raises \u00b6 ValueError If the function is a classmethod, staticmethod, property, or a method of a class. Source code in llm_strategy/llm_function.py 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 def apply_decorator ( f : F_types , decorator ) -> F_types : \"\"\" Apply a decorator to a function. This function is used to apply a decorator to a function, while preserving the function type. This is useful when we want to apply a decorator to a function that is a classmethod, staticmethod, property, or a method of a class. Parameters ---------- f: F_types The function to decorate. decorator: Callable The decorator to apply. Returns ------- F_types The decorated function. Raises ------ ValueError If the function is a classmethod, staticmethod, property, or a method of a class. \"\"\" specific_llm_function : object if isinstance ( f , classmethod ): raise ValueError ( \"Cannot decorate classmethod with llm_strategy (no translation of cls: type atm).\" ) elif isinstance ( f , staticmethod ): specific_llm_function = staticmethod ( apply_decorator ( f . __func__ , decorator )) elif isinstance ( f , property ): specific_llm_function = property ( apply_decorator ( f . fget , decorator ), doc = f . __doc__ ) elif isinstance ( f , types . MethodType ): specific_llm_function = types . MethodType ( apply_decorator ( f . __func__ , decorator ), f . __self__ ) elif hasattr ( f , \"__wrapped__\" ): return apply_decorator ( f . __wrapped__ , decorator ) elif isinstance ( f , LLMFunctionInterface ): specific_llm_function = f elif not callable ( f ): raise ValueError ( f \"Cannot decorate { f } with llm_strategy.\" ) else : if not is_not_implemented ( f ): raise ValueError ( \"The function must not be implemented.\" ) specific_llm_function = track_hyperparameters ( functools . wraps ( f )( decorator ( f ))) return typing . cast ( F_types , specific_llm_function ) get_concise_type_repr ( return_type ) \u00b6 Return a shorter (string) representation of the return type. Examples: <class 'str'> -> str <class 'int'> -> int <class 'CustomType'> -> CustomType <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] For generic types, we want to keep the type arguments as well. <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] <class 'PydanticGenericModel[typing.Dict[str, int]]'> -> PydanticGenericModel[Dict[str, int]] For unspecialized generic types, we want to keep the type arguments as well. so for class PydanticGenericModel(Generic[T]): pass: -> PydanticGenericModel[T] Source code in llm_strategy/llm_function.py 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 def get_concise_type_repr ( return_type : type ): \"\"\"Return a shorter (string) representation of the return type. Examples: <class 'str'> -> str <class 'int'> -> int <class 'CustomType'> -> CustomType <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] For generic types, we want to keep the type arguments as well. <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] <class 'PydanticGenericModel[typing.Dict[str, int]]'> -> PydanticGenericModel[Dict[str, int]] For unspecialized generic types, we want to keep the type arguments as well. so for class PydanticGenericModel(Generic[T]): pass: -> PydanticGenericModel[T] \"\"\" assert isinstance ( return_type , type | types . GenericAlias | _typing_GenericAlias | typing . TypeVar ), return_type name = return_type . __name__ # is it a specialized generic type? if hasattr ( return_type , \"__origin__\" ): origin = return_type . __origin__ if origin is not None : # is it a generic type with type arguments? if hasattr ( return_type , \"__args__\" ): args = return_type . __args__ if args : # is it a generic type with type arguments? args_str = \", \" . join ([ get_concise_type_repr ( arg ) for arg in args ]) return f \" { origin . __name__ } [ { args_str } ]\" # is it a unspecialized generic type? if hasattr ( return_type , \"__parameters__\" ): parameters = return_type . __parameters__ if parameters : # is it a generic type without type arguments? parameters_str = \", \" . join ([ get_concise_type_repr ( parameter ) for parameter in parameters ]) return f \" { name } [ { parameters_str } ]\" return name get_json_schema_hyperparameters ( schema ) \u00b6 Get the hyperparameters from a JSON schema recursively. The hyperparameters are all fields for keys with \"title\" or \"description\". Source code in llm_strategy/llm_function.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_json_schema_hyperparameters ( schema : dict ): \"\"\" Get the hyperparameters from a JSON schema recursively. The hyperparameters are all fields for keys with \"title\" or \"description\". \"\"\" hyperparameters = {} for key , value in schema . items (): if key == \"description\" : hyperparameters [ key ] = value elif isinstance ( value , dict ): sub_hyperparameters = get_json_schema_hyperparameters ( value ) if sub_hyperparameters : hyperparameters [ key ] = sub_hyperparameters return hyperparameters is_not_implemented ( f ) \u00b6 Check that a function only raises NotImplementedError. Source code in llm_strategy/llm_function.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def is_not_implemented ( f : typing . Callable ) -> bool : \"\"\"Check that a function only raises NotImplementedError.\"\"\" unwrapped_f = unwrap_function ( f ) if not hasattr ( unwrapped_f , \"__code__\" ): raise ValueError ( f \"Cannot check whether { f } is implemented. Where is __code__?\" ) # Inspect the opcodes code = unwrapped_f . __code__ # Get the opcodes opcodes = list ( dis . get_instructions ( code )) # Check that it only uses the following opcodes: # - RESUME # - LOAD_GLOBAL # - PRECALL # - CALL # - RAISE_VARARGS valid_opcodes = { \"RESUME\" , \"LOAD_GLOBAL\" , \"PRECALL\" , \"CALL\" , \"RAISE_VARARGS\" , } # We allow at most a function of length len(valid_opcodes) if len ( opcodes ) > len ( valid_opcodes ): return False for opcode in opcodes : if opcode . opname not in valid_opcodes : return False # Check that the function only raises NotImplementedError if opcode . opname == \"LOAD_GLOBAL\" and opcode . argval != \"NotImplementedError\" : return False if opcode . opname == \"RAISE_VARARGS\" and opcode . argval != 1 : return False valid_opcodes . remove ( opcode . opname ) # Check that the function raises a NotImplementedError at the end. if opcodes [ - 1 ] . opname != \"RAISE_VARARGS\" : return False return True llm_explicit_function ( f ) \u00b6 Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. Source code in llm_strategy/llm_function.py 895 896 897 898 899 900 901 902 903 def llm_explicit_function ( f : F_types ) -> F_types : \"\"\" Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. \"\"\" return apply_decorator ( f , lambda f : LLMExplicitFunction ()) llm_function ( f ) \u00b6 Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. Source code in llm_strategy/llm_function.py 906 907 908 909 910 911 912 913 914 def llm_function ( f : F_types ) -> F_types : \"\"\" Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. \"\"\" return apply_decorator ( f , lambda f : LLMFunction ()) update_json_schema_hyperparameters ( schema , hyperparameters ) \u00b6 Nested merge of the schema dict with the hyperparameters dict. Source code in llm_strategy/llm_function.py 56 57 58 59 60 61 62 63 64 65 66 67 def update_json_schema_hyperparameters ( schema : dict , hyperparameters : dict ): \"\"\" Nested merge of the schema dict with the hyperparameters dict. \"\"\" for key , value in hyperparameters . items (): if key in schema : if isinstance ( value , dict ): update_json_schema_hyperparameters ( schema [ key ], value ) else : schema [ key ] = value else : schema [ key ] = value","title":"llm_function"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature","text":"A function call that can be used to generate a prompt. Source code in llm_strategy/llm_function.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @dataclass class LLMBoundSignature : \"\"\" A function call that can be used to generate a prompt. \"\"\" structured_prompt : LLMStructuredPrompt signature : inspect . Signature @property def input_type ( self ) -> type [ BaseModel ]: \"\"\"Return the input type.\"\"\" return self . structured_prompt . input_type @property def output_type ( self ) -> type [ BaseModel ]: \"\"\"Return the output type.\"\"\" return self . structured_prompt . output_type @property def docstring ( self ) -> str : \"\"\"Return the docstring.\"\"\" return self . structured_prompt . docstring @property def return_annotation ( self ) -> str : \"\"\"Return the name.\"\"\" return self . structured_prompt . return_annotation def get_input_object ( self , * args : P . args , ** kwargs : P . kwargs ) -> BaseModel : \"\"\"Call the function and return the inputs.\"\"\" # bind the inputs to the signature bound_arguments = LLMBoundSignature . bind ( self . signature , args , kwargs ) # get the arguments arguments = bound_arguments . arguments inputs = self . structured_prompt . input_type ( ** arguments ) return inputs @staticmethod def from_call ( f : typing . Callable [ P , T ], args : P . args , kwargs : P . kwargs ) -> \"LLMBoundSignature\" : # noqa: C901 \"\"\"Create an LLMBoundSignature from a function. Args: f: The function to create the LLMBoundSignature from. args: The positional arguments to the function (but excluding the language model/first param). kwargs: The keyword arguments to the function. \"\"\" # get clean docstring docstring = inspect . getdoc ( f ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( f , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) return_type = signature . return_annotation if return_type is inspect . Parameter . empty : raise ValueError ( \"The function must have a return type.\" ) # create a pydantic model from the parameters parameter_dict = LLMBoundSignature . parameter_items_to_field_tuple ( parameters_items [ 1 :]) # turn function name into a class name class_name = string . capwords ( f . __name__ , sep = \"_\" ) . replace ( \"_\" , \"\" ) # create the input model # collect all __parameters__ from the type annotations # this is necessary because we need to know the types of the parameters # to create the pydantic model generic_parameters : set [ typing . TypeVar ] = set () for parameter in parameter_dict . values (): annotation = parameter [ 0 ] # unwrap any Annotated types while hasattr ( annotation , \"__metadata__\" ): annotation = annotation . __origin__ # if the annotation is already a type variable, add it to the set if isinstance ( annotation , typing . TypeVar ): generic_parameters . add ( annotation ) # if the annotation is a generic type, add the parameters to the set if hasattr ( annotation , \"__parameters__\" ): generic_parameters . update ( annotation . __parameters__ ) model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) if generic_parameters : bases = ( pydantic . generics . GenericModel , typing . Generic [ * generic_parameters ]) input_type = create_model ( f \" { class_name } Inputs\" , __base__ = bases , __module__ = f . __module__ , ** model_spec ) else : input_type = create_model ( f \" { class_name } Inputs\" , __module__ = f . __module__ , ** model_spec ) input_type . update_forward_refs () # update parameter_dict types with bound_arguments # this ensures that we serialize the actual types # might not be optimal because the language model won't be aware of original types, however bound_arguments = LLMBoundSignature . bind ( signature , args , kwargs ) for parameter_name in parameter_dict : if parameter_name in bound_arguments . arguments : parameter_dict [ parameter_name ] = ( type ( bound_arguments . arguments [ parameter_name ]), parameter_dict [ parameter_name ][ 1 ], ) specific_model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) specific_input_type = create_model ( f \"Specific { class_name } Inputs\" , __module__ = f . __module__ , ** specific_model_spec ) specific_input_type . update_forward_refs () input = specific_input_type ( ** bound_arguments . arguments ) llm_structured_prompt : LLMStructuredPrompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = input_type , return_annotation = return_type , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) @staticmethod def parameter_items_to_field_tuple ( parameters_items : list [ tuple [ str , inspect . Parameter ]]): \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , parameter in parameters_items : # every parameter must be annotated or have a default value annotation = parameter . annotation if annotation is type : annotation = TyperWrapper if parameter . default is inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , ... ) else : parameter_dict [ parameter_name ] = ( annotation , parameter . default ) return parameter_dict @staticmethod def field_tuples_to_model_spec ( field_tuples_dict : dict [ str , tuple [ str , tuple [ type , ... ]]] ) -> dict [ str , tuple [ type , object ] | object ]: \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , ( annotation , default ) in field_tuples_dict . items (): # every parameter must be annotated or have a default value if default is ... : parameter_dict [ parameter_name ] = ( annotation , ... ) else : if annotation is not inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , default ) else : parameter_dict [ parameter_name ] = default return parameter_dict @staticmethod def get_or_create_pydantic_default ( field : FieldInfo ): if field . default is not Undefined : if field . default is Ellipsis : return inspect . Parameter . empty return field . default if field . default_factory is not None : return field . default_factory () return None @staticmethod def bind ( signature , args , kwargs ): \"\"\" Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. \"\"\" # resolve parameter defaults to FieldInfo.default if the parameter is a field signature_fixed_defaults = signature . replace ( parameters = [ parameter . replace ( default = LLMBoundSignature . get_or_create_pydantic_default ( parameter . default )) if isinstance ( parameter . default , FieldInfo ) else parameter for parameter in list ( signature . parameters . values ())[ 1 :] ] ) bound_arguments = signature_fixed_defaults . bind ( * args , ** kwargs ) bound_arguments . apply_defaults () return bound_arguments","title":"LLMBoundSignature"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.docstring","text":"Return the docstring.","title":"docstring"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.input_type","text":"Return the input type.","title":"input_type"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.output_type","text":"Return the output type.","title":"output_type"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.return_annotation","text":"Return the name.","title":"return_annotation"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.bind","text":"Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. Source code in llm_strategy/llm_function.py 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 @staticmethod def bind ( signature , args , kwargs ): \"\"\" Bind function taking into account Field definitions and defaults. The first parameter from the original signature is dropped (as it is the language model or chat chain). args and kwargs are bound to the remaining parameters. \"\"\" # resolve parameter defaults to FieldInfo.default if the parameter is a field signature_fixed_defaults = signature . replace ( parameters = [ parameter . replace ( default = LLMBoundSignature . get_or_create_pydantic_default ( parameter . default )) if isinstance ( parameter . default , FieldInfo ) else parameter for parameter in list ( signature . parameters . values ())[ 1 :] ] ) bound_arguments = signature_fixed_defaults . bind ( * args , ** kwargs ) bound_arguments . apply_defaults () return bound_arguments","title":"bind()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.field_tuples_to_model_spec","text":"Get the parameter definitions for a function call from the parameters and arguments. Source code in llm_strategy/llm_function.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 @staticmethod def field_tuples_to_model_spec ( field_tuples_dict : dict [ str , tuple [ str , tuple [ type , ... ]]] ) -> dict [ str , tuple [ type , object ] | object ]: \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , ( annotation , default ) in field_tuples_dict . items (): # every parameter must be annotated or have a default value if default is ... : parameter_dict [ parameter_name ] = ( annotation , ... ) else : if annotation is not inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , default ) else : parameter_dict [ parameter_name ] = default return parameter_dict","title":"field_tuples_to_model_spec()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.from_call","text":"Create an LLMBoundSignature from a function. Parameters: Name Type Description Default f Callable [ P , T ] The function to create the LLMBoundSignature from. required args args The positional arguments to the function (but excluding the language model/first param). required kwargs kwargs The keyword arguments to the function. required Source code in llm_strategy/llm_function.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 @staticmethod def from_call ( f : typing . Callable [ P , T ], args : P . args , kwargs : P . kwargs ) -> \"LLMBoundSignature\" : # noqa: C901 \"\"\"Create an LLMBoundSignature from a function. Args: f: The function to create the LLMBoundSignature from. args: The positional arguments to the function (but excluding the language model/first param). kwargs: The keyword arguments to the function. \"\"\" # get clean docstring docstring = inspect . getdoc ( f ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( f , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) return_type = signature . return_annotation if return_type is inspect . Parameter . empty : raise ValueError ( \"The function must have a return type.\" ) # create a pydantic model from the parameters parameter_dict = LLMBoundSignature . parameter_items_to_field_tuple ( parameters_items [ 1 :]) # turn function name into a class name class_name = string . capwords ( f . __name__ , sep = \"_\" ) . replace ( \"_\" , \"\" ) # create the input model # collect all __parameters__ from the type annotations # this is necessary because we need to know the types of the parameters # to create the pydantic model generic_parameters : set [ typing . TypeVar ] = set () for parameter in parameter_dict . values (): annotation = parameter [ 0 ] # unwrap any Annotated types while hasattr ( annotation , \"__metadata__\" ): annotation = annotation . __origin__ # if the annotation is already a type variable, add it to the set if isinstance ( annotation , typing . TypeVar ): generic_parameters . add ( annotation ) # if the annotation is a generic type, add the parameters to the set if hasattr ( annotation , \"__parameters__\" ): generic_parameters . update ( annotation . __parameters__ ) model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) if generic_parameters : bases = ( pydantic . generics . GenericModel , typing . Generic [ * generic_parameters ]) input_type = create_model ( f \" { class_name } Inputs\" , __base__ = bases , __module__ = f . __module__ , ** model_spec ) else : input_type = create_model ( f \" { class_name } Inputs\" , __module__ = f . __module__ , ** model_spec ) input_type . update_forward_refs () # update parameter_dict types with bound_arguments # this ensures that we serialize the actual types # might not be optimal because the language model won't be aware of original types, however bound_arguments = LLMBoundSignature . bind ( signature , args , kwargs ) for parameter_name in parameter_dict : if parameter_name in bound_arguments . arguments : parameter_dict [ parameter_name ] = ( type ( bound_arguments . arguments [ parameter_name ]), parameter_dict [ parameter_name ][ 1 ], ) specific_model_spec = LLMBoundSignature . field_tuples_to_model_spec ( parameter_dict ) specific_input_type = create_model ( f \"Specific { class_name } Inputs\" , __module__ = f . __module__ , ** specific_model_spec ) specific_input_type . update_forward_refs () input = specific_input_type ( ** bound_arguments . arguments ) llm_structured_prompt : LLMStructuredPrompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = input_type , return_annotation = return_type , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature )","title":"from_call()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.get_input_object","text":"Call the function and return the inputs. Source code in llm_strategy/llm_function.py 517 518 519 520 521 522 523 524 def get_input_object ( self , * args : P . args , ** kwargs : P . kwargs ) -> BaseModel : \"\"\"Call the function and return the inputs.\"\"\" # bind the inputs to the signature bound_arguments = LLMBoundSignature . bind ( self . signature , args , kwargs ) # get the arguments arguments = bound_arguments . arguments inputs = self . structured_prompt . input_type ( ** arguments ) return inputs","title":"get_input_object()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMBoundSignature.parameter_items_to_field_tuple","text":"Get the parameter definitions for a function call from the parameters and arguments. Source code in llm_strategy/llm_function.py 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 @staticmethod def parameter_items_to_field_tuple ( parameters_items : list [ tuple [ str , inspect . Parameter ]]): \"\"\" Get the parameter definitions for a function call from the parameters and arguments. \"\"\" parameter_dict : dict = {} for parameter_name , parameter in parameters_items : # every parameter must be annotated or have a default value annotation = parameter . annotation if annotation is type : annotation = TyperWrapper if parameter . default is inspect . Parameter . empty : parameter_dict [ parameter_name ] = ( annotation , ... ) else : parameter_dict [ parameter_name ] = ( annotation , parameter . default ) return parameter_dict","title":"parameter_items_to_field_tuple()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMExplicitFunction","text":"Bases: LLMFunctionInterface [ P , T ] , Generic [ P , T ] A callable that can be called with a chat model. Source code in llm_strategy/llm_function.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 class LLMExplicitFunction ( LLMFunctionInterface [ P , T ], typing . Generic [ P , T ]): \"\"\" A callable that can be called with a chat model. \"\"\" def llm_bound_signature ( self , input : BaseModel ) -> LLMBoundSignature : \"\"\"Create an LLMFunctionSpec from a function.\"\"\" # get clean docstring of docstring = inspect . getdoc ( self ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( self , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) second_parameter : inspect . Parameter = parameters_items [ 1 ][ 1 ] llm_structured_prompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = second_parameter . annotation , return_annotation = signature . return_annotation , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature ) def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) def __getattr__ ( self , item ): return getattr ( self . __wrapped__ , item ) @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = True , capture_args = slicer [ 1 :]) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input : BaseModel ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , input ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = self . llm_bound_signature ( input ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value","title":"LLMExplicitFunction"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMExplicitFunction.__call__","text":"Call the function. Source code in llm_strategy/llm_function.py 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = True , capture_args = slicer [ 1 :]) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input : BaseModel ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , input ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = self . llm_bound_signature ( input ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value","title":"__call__()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMExplicitFunction.__get__","text":"Support instance methods. Source code in llm_strategy/llm_function.py 795 796 797 798 799 800 801 def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance )","title":"__get__()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMExplicitFunction.llm_bound_signature","text":"Create an LLMFunctionSpec from a function. Source code in llm_strategy/llm_function.py 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 def llm_bound_signature ( self , input : BaseModel ) -> LLMBoundSignature : \"\"\"Create an LLMFunctionSpec from a function.\"\"\" # get clean docstring of docstring = inspect . getdoc ( self ) if docstring is None : raise ValueError ( \"The function must have a docstring.\" ) # get the type of the first argument signature = inspect . signature ( self , eval_str = True ) # get all parameters parameters_items : list [ tuple [ str , inspect . Parameter ]] = list ( signature . parameters . items ()) # check that there is at least one parameter if not parameters_items : raise ValueError ( \"The function must have at least one parameter.\" ) # check that the first parameter has a type annotation that is an instance of BaseLanguageModel # or a TrackedChatChain first_parameter : inspect . Parameter = parameters_items [ 0 ][ 1 ] if first_parameter . annotation is not inspect . Parameter . empty : if not issubclass ( first_parameter . annotation , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) second_parameter : inspect . Parameter = parameters_items [ 1 ][ 1 ] llm_structured_prompt = LLMStructuredPrompt . create ( docstring = docstring , input_type = second_parameter . annotation , return_annotation = signature . return_annotation , input = input , ) return LLMBoundSignature ( llm_structured_prompt , signature )","title":"llm_bound_signature()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMFunction","text":"Bases: LLMFunctionInterface [ P , T ] , Generic [ P , T ] A callable that can be called with a chat model. Source code in llm_strategy/llm_function.py 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 class LLMFunction ( LLMFunctionInterface [ P , T ], typing . Generic [ P , T ]): \"\"\" A callable that can be called with a chat model. \"\"\" def llm_bound_signature ( self , * args , ** kwargs ) -> LLMBoundSignature : return LLMBoundSignature . from_call ( self , args , kwargs ) def get_input_object ( self , * args , ** kwargs ) -> BaseModel : return self . llm_bound_signature ( * args , ** kwargs ) . get_input_object ( * args , ** kwargs ) def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance ) def __getattr__ ( self , item ): return getattr ( self . __wrapped__ , item ) def explicit ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input_object : BaseModel ): \"\"\"Call the function with explicit inputs.\"\"\" return self ( language_model_or_chat_chain , ** dict ( input_object )) @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = slicer [ 1 :], capture_args = True ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , * args : P . args , ** kwargs : P . kwargs , ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , * args , ** kwargs ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = LLMBoundSignature . from_call ( self , args , kwargs ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value","title":"LLMFunction"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMFunction.__call__","text":"Call the function. Source code in llm_strategy/llm_function.py 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 @trace_calls ( kind = TraceNodeKind . CHAIN , capture_return = slicer [ 1 :], capture_args = True ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , * args : P . args , ** kwargs : P . kwargs , ) -> T : \"\"\"Call the function.\"\"\" update_name ( self . __name__ ) # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # We expect that we wrap a function that raises NotImplementedError # We call it, so we can set breakpoints in the function try : self . __wrapped__ ( language_model_or_chat_chain , * args , ** kwargs ) raise ValueError ( \"The function must raise NotImplementedError.\" ) except NotImplementedError : pass llm_bound_signature = LLMBoundSignature . from_call ( self , args , kwargs ) return_value = llm_bound_signature . structured_prompt ( language_model_or_chat_chain ) return return_value","title":"__call__()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMFunction.__get__","text":"Support instance methods. Source code in llm_strategy/llm_function.py 714 715 716 717 718 719 720 def __get__ ( self , instance : object , owner : type | None = None ) -> typing . Callable : \"\"\"Support instance methods.\"\"\" if instance is None : return self # Bind self to instance as MethodType return types . MethodType ( self , instance )","title":"__get__()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMFunction.explicit","text":"Call the function with explicit inputs. Source code in llm_strategy/llm_function.py 725 726 727 728 def explicit ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , input_object : BaseModel ): \"\"\"Call the function with explicit inputs.\"\"\" return self ( language_model_or_chat_chain , ** dict ( input_object ))","title":"explicit()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMStructuredPrompt","text":"Bases: Generic [ B , T ] A structured prompt for a language model. Source code in llm_strategy/llm_function.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 @dataclass class LLMStructuredPrompt ( typing . Generic [ B , T ]): \"\"\" A structured prompt for a language model. \"\"\" docstring : str input_type : type [ B ] output_type : type [ Output [ T ]] return_annotation : T input : B @staticmethod def extract_from_definitions ( definitions : dict , type_ : type ) -> dict : normalized_name = pydantic . schema . normalize_name ( type_ . __name__ ) sub_schema = definitions [ normalized_name ] del definitions [ normalized_name ] return sub_schema def get_json_schema ( self , exclude_default : bool = True ) -> dict : schema = pydantic . schema . schema ([ self . input_type , self . output_type ], ref_template = \" {model} \" ) definitions : dict = deepcopy ( schema [ \"definitions\" ]) # remove title and type from each sub dict in the definitions for value in definitions . values (): value . pop ( \"title\" ) value . pop ( \"type\" ) for property in value . get ( \"properties\" , {}) . values (): property . pop ( \"title\" , None ) if exclude_default : property . pop ( \"default\" , None ) input_schema = self . extract_from_definitions ( definitions , self . input_type ) output_schema = self . extract_from_definitions ( definitions , self . output_type ) schema = dict ( input_schema = input_schema , output_schema = output_schema , additional_definitions = definitions , ) return schema @staticmethod def create ( docstring : str , input_type : type [ B ], return_annotation : T , input : B ) -> \"LLMStructuredPrompt[B, T]\" : \"\"\"Create an LLMExplicitFunction.\"\"\" # determine the return type # the return type can be a type annotation or an Annotated type with annotation being a FieldInfo if typing . get_origin ( return_annotation ) is typing . Annotated : return_info = typing . get_args ( return_annotation ) else : return_info = ( return_annotation , ... ) # resolve generic types generic_type_map = LLMStructuredPrompt . resolve_generic_types ( input_type , input ) return_type : type = LLMStructuredPrompt . resolve_type ( return_info [ 0 ], generic_type_map ) if return_type is types . NoneType : # noqa: E721 raise ValueError ( f \"Resolve return type { return_info [ 0 ] } is None! This would be a NOP.\" ) return_info = ( return_type , return_info [ 1 ]) if typing . get_origin ( return_annotation ) is typing . Annotated : assert hasattr ( return_annotation , \"copy_with\" ) resolved_return_annotation = return_annotation . copy_with ([ return_info [ 0 ]]) else : resolved_return_annotation = return_info [ 0 ] # create the output model resolved_output_model_type = Output [ return_type ] # noqa # resolve input_type resolved_input_type = LLMStructuredPrompt . resolve_type ( input_type , generic_type_map ) return LLMStructuredPrompt ( docstring = docstring , input_type = resolved_input_type , output_type = resolved_output_model_type , return_annotation = resolved_return_annotation , input = input , ) @staticmethod def resolve_type ( source_type : type , generic_type_map : dict [ type , type ]) -> type : \"\"\" Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. \"\"\" if source_type in generic_type_map : source_type = generic_type_map [ source_type ] if isinstance ( source_type , type ) and issubclass ( source_type , generics . GenericModel ): base_generic_type = LLMStructuredPrompt . get_base_generic_type ( source_type ) generic_parameter_type_map = LLMStructuredPrompt . get_generic_type_map ( source_type , base_generic_type ) # forward step using the generic type map resolved_generic_type_map = { generic_type : generic_type_map . get ( target_type , target_type ) for generic_type , target_type in generic_parameter_type_map . items () } resolved_tuple = tuple ( resolved_generic_type_map [ generic_type ] for generic_type in base_generic_type . __parameters__ ) source_type = base_generic_type [ resolved_tuple ] else : # we let Pydantic handle the rest source_type = replace_types ( source_type , generic_type_map ) return source_type @staticmethod def resolve_generic_types ( model : type [ BaseModel ], instance : BaseModel ): generic_type_map : dict = {} for field_name , attr_value in list ( instance ): if field_name not in model . __annotations__ : continue annotation = model . __annotations__ [ field_name ] # if the annotation is an Annotated type, get the type annotation if typing . get_origin ( annotation ) is typing . Annotated : annotation = typing . get_args ( annotation )[ 0 ] # if the annotation is a type var, resolve it into the generic type map if isinstance ( annotation , typing . TypeVar ): LLMStructuredPrompt . add_resolved_type ( generic_type_map , annotation , type ( attr_value )) # if the annotation is a generic type alias ignore elif isinstance ( annotation , types . GenericAlias ): continue # if the annotation is a type, check if it is a generic type elif issubclass ( annotation , generics . GenericModel ): # check if the type is in generics._assigned_parameters generic_definition_type_map = LLMStructuredPrompt . get_generic_type_map ( annotation ) argument_type = type ( attr_value ) generic_instance_type_map = LLMStructuredPrompt . get_generic_type_map ( argument_type ) assert list ( generic_definition_type_map . keys ()) == list ( generic_instance_type_map . keys ()) # update the generic type map # if the generic type is already in the map, check that it is the same for generic_parameter , generic_parameter_target in generic_definition_type_map . items (): if generic_parameter_target not in annotation . __parameters__ : continue resolved_type = generic_instance_type_map [ generic_parameter ] LLMStructuredPrompt . add_resolved_type ( generic_type_map , generic_parameter_target , resolved_type ) return generic_type_map @staticmethod def add_resolved_type ( generic_type_map , source_type , resolved_type ): \"\"\" Add a resolved type to the generic type map. \"\"\" if source_type in generic_type_map : # TODO: support finding the common base class? if ( previous_resolution := generic_type_map [ source_type ]) is not resolved_type : raise ValueError ( f \"Cannot resolve generic type { source_type } , conflicting \" f \"resolution: { previous_resolution } and { resolved_type } .\" ) else : generic_type_map [ source_type ] = resolved_type @staticmethod def get_generic_type_map ( generic_type , base_generic_type = None ): if base_generic_type is None : base_generic_type = LLMStructuredPrompt . get_base_generic_type ( generic_type ) base_classes = inspect . getmro ( generic_type ) # we have to iterate through the base classes generic_parameter_type_map = { generic_type : generic_type for generic_type in generic_type . __parameters__ } for base_class in base_classes : # skip baseclasses that are from pydantic.generic # this avoids a bug that is caused by generics.GenericModel.__parameterized_bases_ if base_class . __module__ == \"pydantic.generics\" : continue if issubclass ( base_class , base_generic_type ): if base_class in generics . _assigned_parameters : assignment = generics . _assigned_parameters [ base_class ] generic_parameter_type_map = { old_generic_type : generic_parameter_type_map . get ( new_generic_type , new_generic_type ) for old_generic_type , new_generic_type in assignment . items () } return generic_parameter_type_map @staticmethod def get_base_generic_type ( field_type ) -> type [ generics . GenericModel ]: # get the base class name from annotation (which is without []) base_generic_name = field_type . __name__ if \"[\" in field_type . __name__ : base_generic_name = field_type . __name__ . split ( \"[\" )[ 0 ] # get the base class from argument_type_base_classes with base_generic_name for base_class in reversed ( inspect . getmro ( field_type )): if base_class . __name__ == base_generic_name and issubclass ( field_type , base_class ): base_generic_type = base_class break else : raise ValueError ( f \"Could not find base generic type { base_generic_name } for { field_type } .\" ) return base_generic_type @trace_calls ( name = \"LLMStructuredPrompt\" , kind = TraceNodeKind . CHAIN , capture_args = False , capture_return = False ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , ) -> T : \"\"\"Call the function.\"\"\" # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # get the input and output schema as JSON dict schema = self . get_json_schema () # print(json.dumps(schema, indent=1)) update_json_schema_hyperparameters ( schema , Hyperparameter ( \"json_schema\" ) @ get_json_schema_hyperparameters ( schema ), ) update_event_properties ( dict ( arguments = dict ( self . input ), ) ) parsed_output = self . query ( language_model_or_chat_chain , schema ) # print(f\"Input: {self.input.json(indent=1)}\") # print(f\"Output: {json.dumps(json.loads(parsed_output.json())['return_value'], indent=1)}\") update_event_properties ( dict ( result = parsed_output . return_value )) return parsed_output . return_value @track_hyperparameters def query ( self , language_model_or_chat_chain , schema ): # noqa: C901 # create the prompt json_dumps_kwargs = Hyperparameter ( \"json_dumps_kwargs\" ) @ dict ( indent = None ) additional_definitions_prompt_template = Hyperparameter ( \"additional_definitions_prompt_template\" , \"Here is the schema for additional data types: \\n ``` \\n {additional_definitions} \\n ``` \\n\\n \" , ) optional_additional_definitions_prompt = \"\" if schema [ \"additional_definitions\" ]: optional_additional_definitions_prompt = additional_definitions_prompt_template . format ( additional_definitions = json . dumps ( schema [ \"additional_definitions\" ], ** json_dumps_kwargs ) ) prompt = ( Hyperparameter ( \"llm_structured_prompt_template\" , description = ( \"The general-purpose prompt for the structured prompt execution. It tells the LLM what to \" \"do and how to read function arguments and structure return values. \" ), ) @ ' {docstring} \\n\\n The input and output are formatted as a JSON interface that conforms to the JSON schemas below. \\n\\n As an example, for the schema {{\"properties\": {{\"foo\": {{\"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}} the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted. \\n\\n {optional_additional_definitions_prompt} Here is the input schema: \\n ``` \\n {input_schema} \\n ``` \\n\\n Here is the output schema: \\n ``` \\n {output_schema} \\n ``` \\n Now output the results for the following inputs: \\n ``` \\n {inputs} \\n ```' ) . format ( docstring = self . docstring , optional_additional_definitions_prompt = optional_additional_definitions_prompt , input_schema = json . dumps ( schema [ \"input_schema\" ], ** json_dumps_kwargs ), output_schema = json . dumps ( schema [ \"output_schema\" ], ** json_dumps_kwargs ), inputs = self . input . json ( ** json_dumps_kwargs ), ) # get the response num_retries = Hyperparameter ( \"num_retries_on_parser_failure\" ) @ 3 if language_model_or_chat_chain is None : raise ValueError ( \"The language model or chat chain must be provided.\" ) if isinstance ( language_model_or_chat_chain , BaseChatModel ): language_model_or_chat_chain = ChatChain ( language_model_or_chat_chain , []) if isinstance ( language_model_or_chat_chain , ChatChain ): chain = language_model_or_chat_chain for _ in range ( num_retries ): output , chain = chain . query ( prompt , model_args = chain . enforce_json_response ()) try : parsed_output = parse ( output , self . output_type ) break except OutputParserException as e : prompt = ( Hyperparameter ( \"error_prompt\" ) @ \"Tried to parse your output but failed: \\n\\n \" + str ( e ) + Hyperparameter ( \"retry_prompt\" ) @ \" \\n\\n Please try again and avoid this issue.\" ) else : exception = OutputParserException ( f \"Failed to parse the output after { num_retries } retries.\" ) exception . add_note ( chain ) raise exception elif isinstance ( language_model_or_chat_chain , BaseLLM ): model : BaseChatModel = language_model_or_chat_chain # Check if the language model is of type \"openai\" and extend model args with a response format in that case model_dict = model . dict () if \"openai\" in model_dict [ \"_type\" ] and model_dict . get ( \"model_name\" ) in ( \"gpt-4-1106-preview\" , \"gpt-3.5-turbo-1106\" , ): model_args = dict ( response_format = dict ( type = \"json_object\" )) else : model_args = {} for _ in range ( num_retries ): output = model ( prompt , ** model_args ) try : parsed_output = parse ( output , self . output_type ) break except OutputParserException as e : prompt = ( prompt + Hyperparameter ( \"output_prompt\" ) @ \" \\n\\n Received the output \\n\\n \" + output + Hyperparameter ( \"error_prompt\" ) @ \"Tried to parse your output but failed: \\n\\n \" + str ( e ) + Hyperparameter ( \"retry_prompt\" ) @ \" \\n\\n Please try again and avoid this issue.\" ) else : exception = OutputParserException ( f \"Failed to parse the output after { num_retries } retries.\" ) exception . add_note ( prompt ) raise exception else : raise ValueError ( \"The language model or chat chain must be provided.\" ) return parsed_output","title":"LLMStructuredPrompt"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMStructuredPrompt.__call__","text":"Call the function. Source code in llm_strategy/llm_function.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 @trace_calls ( name = \"LLMStructuredPrompt\" , kind = TraceNodeKind . CHAIN , capture_args = False , capture_return = False ) def __call__ ( self , language_model_or_chat_chain : BaseLanguageModel | ChatChain , ) -> T : \"\"\"Call the function.\"\"\" # check that the first argument is an instance of BaseLanguageModel # or a TrackedChatChain or UntrackedChatChain if not isinstance ( language_model_or_chat_chain , BaseLanguageModel | ChatChain ): raise ValueError ( \"The first parameter must be an instance of BaseLanguageModel or ChatChain.\" ) # get the input and output schema as JSON dict schema = self . get_json_schema () # print(json.dumps(schema, indent=1)) update_json_schema_hyperparameters ( schema , Hyperparameter ( \"json_schema\" ) @ get_json_schema_hyperparameters ( schema ), ) update_event_properties ( dict ( arguments = dict ( self . input ), ) ) parsed_output = self . query ( language_model_or_chat_chain , schema ) # print(f\"Input: {self.input.json(indent=1)}\") # print(f\"Output: {json.dumps(json.loads(parsed_output.json())['return_value'], indent=1)}\") update_event_properties ( dict ( result = parsed_output . return_value )) return parsed_output . return_value","title":"__call__()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMStructuredPrompt.add_resolved_type","text":"Add a resolved type to the generic type map. Source code in llm_strategy/llm_function.py 305 306 307 308 309 310 311 312 313 314 315 316 317 318 @staticmethod def add_resolved_type ( generic_type_map , source_type , resolved_type ): \"\"\" Add a resolved type to the generic type map. \"\"\" if source_type in generic_type_map : # TODO: support finding the common base class? if ( previous_resolution := generic_type_map [ source_type ]) is not resolved_type : raise ValueError ( f \"Cannot resolve generic type { source_type } , conflicting \" f \"resolution: { previous_resolution } and { resolved_type } .\" ) else : generic_type_map [ source_type ] = resolved_type","title":"add_resolved_type()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMStructuredPrompt.create","text":"Create an LLMExplicitFunction. Source code in llm_strategy/llm_function.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 @staticmethod def create ( docstring : str , input_type : type [ B ], return_annotation : T , input : B ) -> \"LLMStructuredPrompt[B, T]\" : \"\"\"Create an LLMExplicitFunction.\"\"\" # determine the return type # the return type can be a type annotation or an Annotated type with annotation being a FieldInfo if typing . get_origin ( return_annotation ) is typing . Annotated : return_info = typing . get_args ( return_annotation ) else : return_info = ( return_annotation , ... ) # resolve generic types generic_type_map = LLMStructuredPrompt . resolve_generic_types ( input_type , input ) return_type : type = LLMStructuredPrompt . resolve_type ( return_info [ 0 ], generic_type_map ) if return_type is types . NoneType : # noqa: E721 raise ValueError ( f \"Resolve return type { return_info [ 0 ] } is None! This would be a NOP.\" ) return_info = ( return_type , return_info [ 1 ]) if typing . get_origin ( return_annotation ) is typing . Annotated : assert hasattr ( return_annotation , \"copy_with\" ) resolved_return_annotation = return_annotation . copy_with ([ return_info [ 0 ]]) else : resolved_return_annotation = return_info [ 0 ] # create the output model resolved_output_model_type = Output [ return_type ] # noqa # resolve input_type resolved_input_type = LLMStructuredPrompt . resolve_type ( input_type , generic_type_map ) return LLMStructuredPrompt ( docstring = docstring , input_type = resolved_input_type , output_type = resolved_output_model_type , return_annotation = resolved_return_annotation , input = input , )","title":"create()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.LLMStructuredPrompt.resolve_type","text":"Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. Source code in llm_strategy/llm_function.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 @staticmethod def resolve_type ( source_type : type , generic_type_map : dict [ type , type ]) -> type : \"\"\" Resolve a type using the generic type map. Supports Pydantic.GenericModel and typing.Generic. \"\"\" if source_type in generic_type_map : source_type = generic_type_map [ source_type ] if isinstance ( source_type , type ) and issubclass ( source_type , generics . GenericModel ): base_generic_type = LLMStructuredPrompt . get_base_generic_type ( source_type ) generic_parameter_type_map = LLMStructuredPrompt . get_generic_type_map ( source_type , base_generic_type ) # forward step using the generic type map resolved_generic_type_map = { generic_type : generic_type_map . get ( target_type , target_type ) for generic_type , target_type in generic_parameter_type_map . items () } resolved_tuple = tuple ( resolved_generic_type_map [ generic_type ] for generic_type in base_generic_type . __parameters__ ) source_type = base_generic_type [ resolved_tuple ] else : # we let Pydantic handle the rest source_type = replace_types ( source_type , generic_type_map ) return source_type","title":"resolve_type()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.TyperWrapper","text":"Bases: str A wrapper around a type that can be used to create a Pydantic model. This is used to support @classmethods. Source code in llm_strategy/llm_function.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class TyperWrapper ( str ): \"\"\" A wrapper around a type that can be used to create a Pydantic model. This is used to support @classmethods. \"\"\" @classmethod def __get_validators__ ( cls ) -> typing . Iterator [ typing . Callable ]: # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def validate ( cls , v : type ) -> str : if not isinstance ( v , type ): raise TypeError ( \"type required\" ) return v . __qualname__","title":"TyperWrapper"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.apply_decorator","text":"Apply a decorator to a function. This function is used to apply a decorator to a function, while preserving the function type. This is useful when we want to apply a decorator to a function that is a classmethod, staticmethod, property, or a method of a class.","title":"apply_decorator()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.apply_decorator--parameters","text":"F_types The function to decorate. decorator: Callable The decorator to apply.","title":"Parameters"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.apply_decorator--returns","text":"F_types The decorated function.","title":"Returns"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.apply_decorator--raises","text":"ValueError If the function is a classmethod, staticmethod, property, or a method of a class. Source code in llm_strategy/llm_function.py 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 def apply_decorator ( f : F_types , decorator ) -> F_types : \"\"\" Apply a decorator to a function. This function is used to apply a decorator to a function, while preserving the function type. This is useful when we want to apply a decorator to a function that is a classmethod, staticmethod, property, or a method of a class. Parameters ---------- f: F_types The function to decorate. decorator: Callable The decorator to apply. Returns ------- F_types The decorated function. Raises ------ ValueError If the function is a classmethod, staticmethod, property, or a method of a class. \"\"\" specific_llm_function : object if isinstance ( f , classmethod ): raise ValueError ( \"Cannot decorate classmethod with llm_strategy (no translation of cls: type atm).\" ) elif isinstance ( f , staticmethod ): specific_llm_function = staticmethod ( apply_decorator ( f . __func__ , decorator )) elif isinstance ( f , property ): specific_llm_function = property ( apply_decorator ( f . fget , decorator ), doc = f . __doc__ ) elif isinstance ( f , types . MethodType ): specific_llm_function = types . MethodType ( apply_decorator ( f . __func__ , decorator ), f . __self__ ) elif hasattr ( f , \"__wrapped__\" ): return apply_decorator ( f . __wrapped__ , decorator ) elif isinstance ( f , LLMFunctionInterface ): specific_llm_function = f elif not callable ( f ): raise ValueError ( f \"Cannot decorate { f } with llm_strategy.\" ) else : if not is_not_implemented ( f ): raise ValueError ( \"The function must not be implemented.\" ) specific_llm_function = track_hyperparameters ( functools . wraps ( f )( decorator ( f ))) return typing . cast ( F_types , specific_llm_function )","title":"Raises"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.get_concise_type_repr","text":"Return a shorter (string) representation of the return type. Examples: <class 'str'> -> str <class 'int'> -> int <class 'CustomType'> -> CustomType <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] For generic types, we want to keep the type arguments as well. <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] <class 'PydanticGenericModel[typing.Dict[str, int]]'> -> PydanticGenericModel[Dict[str, int]] For unspecialized generic types, we want to keep the type arguments as well. so for class PydanticGenericModel(Generic[T]): pass: -> PydanticGenericModel[T] Source code in llm_strategy/llm_function.py 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 def get_concise_type_repr ( return_type : type ): \"\"\"Return a shorter (string) representation of the return type. Examples: <class 'str'> -> str <class 'int'> -> int <class 'CustomType'> -> CustomType <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] For generic types, we want to keep the type arguments as well. <class 'typing.List[typing.Dict[str, int]]'> -> List[Dict[str, int]] <class 'PydanticGenericModel[typing.Dict[str, int]]'> -> PydanticGenericModel[Dict[str, int]] For unspecialized generic types, we want to keep the type arguments as well. so for class PydanticGenericModel(Generic[T]): pass: -> PydanticGenericModel[T] \"\"\" assert isinstance ( return_type , type | types . GenericAlias | _typing_GenericAlias | typing . TypeVar ), return_type name = return_type . __name__ # is it a specialized generic type? if hasattr ( return_type , \"__origin__\" ): origin = return_type . __origin__ if origin is not None : # is it a generic type with type arguments? if hasattr ( return_type , \"__args__\" ): args = return_type . __args__ if args : # is it a generic type with type arguments? args_str = \", \" . join ([ get_concise_type_repr ( arg ) for arg in args ]) return f \" { origin . __name__ } [ { args_str } ]\" # is it a unspecialized generic type? if hasattr ( return_type , \"__parameters__\" ): parameters = return_type . __parameters__ if parameters : # is it a generic type without type arguments? parameters_str = \", \" . join ([ get_concise_type_repr ( parameter ) for parameter in parameters ]) return f \" { name } [ { parameters_str } ]\" return name","title":"get_concise_type_repr()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.get_json_schema_hyperparameters","text":"Get the hyperparameters from a JSON schema recursively. The hyperparameters are all fields for keys with \"title\" or \"description\". Source code in llm_strategy/llm_function.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_json_schema_hyperparameters ( schema : dict ): \"\"\" Get the hyperparameters from a JSON schema recursively. The hyperparameters are all fields for keys with \"title\" or \"description\". \"\"\" hyperparameters = {} for key , value in schema . items (): if key == \"description\" : hyperparameters [ key ] = value elif isinstance ( value , dict ): sub_hyperparameters = get_json_schema_hyperparameters ( value ) if sub_hyperparameters : hyperparameters [ key ] = sub_hyperparameters return hyperparameters","title":"get_json_schema_hyperparameters()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.is_not_implemented","text":"Check that a function only raises NotImplementedError. Source code in llm_strategy/llm_function.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def is_not_implemented ( f : typing . Callable ) -> bool : \"\"\"Check that a function only raises NotImplementedError.\"\"\" unwrapped_f = unwrap_function ( f ) if not hasattr ( unwrapped_f , \"__code__\" ): raise ValueError ( f \"Cannot check whether { f } is implemented. Where is __code__?\" ) # Inspect the opcodes code = unwrapped_f . __code__ # Get the opcodes opcodes = list ( dis . get_instructions ( code )) # Check that it only uses the following opcodes: # - RESUME # - LOAD_GLOBAL # - PRECALL # - CALL # - RAISE_VARARGS valid_opcodes = { \"RESUME\" , \"LOAD_GLOBAL\" , \"PRECALL\" , \"CALL\" , \"RAISE_VARARGS\" , } # We allow at most a function of length len(valid_opcodes) if len ( opcodes ) > len ( valid_opcodes ): return False for opcode in opcodes : if opcode . opname not in valid_opcodes : return False # Check that the function only raises NotImplementedError if opcode . opname == \"LOAD_GLOBAL\" and opcode . argval != \"NotImplementedError\" : return False if opcode . opname == \"RAISE_VARARGS\" and opcode . argval != 1 : return False valid_opcodes . remove ( opcode . opname ) # Check that the function raises a NotImplementedError at the end. if opcodes [ - 1 ] . opname != \"RAISE_VARARGS\" : return False return True","title":"is_not_implemented()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.llm_explicit_function","text":"Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. Source code in llm_strategy/llm_function.py 895 896 897 898 899 900 901 902 903 def llm_explicit_function ( f : F_types ) -> F_types : \"\"\" Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. \"\"\" return apply_decorator ( f , lambda f : LLMExplicitFunction ())","title":"llm_explicit_function()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.llm_function","text":"Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. Source code in llm_strategy/llm_function.py 906 907 908 909 910 911 912 913 914 def llm_function ( f : F_types ) -> F_types : \"\"\" Decorator to wrap a function with a chat model. f is a function to a dataclass or Pydantic model. The docstring of the function provides instructions for the model. \"\"\" return apply_decorator ( f , lambda f : LLMFunction ())","title":"llm_function()"},{"location":"reference/llm_strategy/llm_function/#llm_strategy.llm_function.update_json_schema_hyperparameters","text":"Nested merge of the schema dict with the hyperparameters dict. Source code in llm_strategy/llm_function.py 56 57 58 59 60 61 62 63 64 65 66 67 def update_json_schema_hyperparameters ( schema : dict , hyperparameters : dict ): \"\"\" Nested merge of the schema dict with the hyperparameters dict. \"\"\" for key , value in hyperparameters . items (): if key in schema : if isinstance ( value , dict ): update_json_schema_hyperparameters ( schema [ key ], value ) else : schema [ key ] = value else : schema [ key ] = value","title":"update_json_schema_hyperparameters()"},{"location":"reference/llm_strategy/llm_strategy/","text":"can_wrap_function_in_llm ( f ) \u00b6 Return True if f can be wrapped in an LLMCall. Source code in llm_strategy/llm_strategy.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def can_wrap_function_in_llm ( f : typing . Callable [ P , T ]) -> bool : \"\"\" Return True if f can be wrapped in an LLMCall. \"\"\" unwrapped = unwrap_function ( f ) if not callable ( unwrapped ): return False if inspect . isgeneratorfunction ( unwrapped ): return False if inspect . iscoroutinefunction ( unwrapped ): return False if not inspect . isfunction ( unwrapped ) and not inspect . ismethod ( unwrapped ): return False return is_not_implemented ( unwrapped ) can_wrap_member_in_llm ( f ) \u00b6 Return True if f can be wrapped in an LLMCall. Source code in llm_strategy/llm_strategy.py 95 96 97 98 99 100 101 102 103 104 def can_wrap_member_in_llm ( f : typing . Callable [ P , T ]) -> bool : \"\"\" Return True if f can be wrapped in an LLMCall. \"\"\" if isinstance ( f , LLMFunction ): return True if dataclasses . is_dataclass ( f ): return True return can_wrap_function_in_llm ( f ) llm_strategy ( llm ) \u00b6 A strategy that implements whatever it decorates (or is called on) using the LLM. Source code in llm_strategy/llm_strategy.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def llm_strategy ( llm : BaseLLM ) -> typing . Callable [[ T ], T ]: # noqa: C901 \"\"\" A strategy that implements whatever it decorates (or is called on) using the LLM. \"\"\" @typing . no_type_check def decorator ( f : T ) -> T : assert can_wrap_member_in_llm ( f ) # For an instance of dataclass, call llm_strategy_dataclass with the fields. if dataclasses . is_dataclass ( f ): if isinstance ( f , type ): return llm_dataclass ( f , llm ) else : implemented_dataclass = llm_dataclass ( type ( f ), llm ) # Create an instance of the implemented dataclass using the fields from f params = { field . name : getattr ( f , field . name ) for field in dataclasses . fields ( f )} return implemented_dataclass ( ** params ) else : def inner_decorator ( unwrapped_f ): llm_f = None @functools . wraps ( unwrapped_f ) def strategy_wrapper ( * args , ** kwargs ): nonlocal llm_f if llm_f is None : # Get the signature of f sig = inspect . signature ( unwrapped_f , eval_str = True ) # Add a llm parameter to the signature as first argument new_params = [ inspect . Parameter ( \"__llm\" , inspect . Parameter . POSITIONAL_ONLY )] new_params . extend ( sig . parameters . values ()) new_sig = sig . replace ( parameters = new_params ) def dummy_f ( * args , ** kwargs ): raise NotImplementedError () new_f = functools . wraps ( unwrapped_f )( dummy_f ) new_f . __module__ = unwrapped_f . __module__ # Set the signature of the new function new_f . __signature__ = new_sig del new_f . __wrapped__ # Wrap the function in an LLMFunction llm_f = functools . wraps ( new_f )( LLMFunction ()) return llm_f ( llm , * args , ** kwargs ) return strategy_wrapper return apply_decorator ( f , inner_decorator ) return decorator","title":"llm_strategy"},{"location":"reference/llm_strategy/llm_strategy/#llm_strategy.llm_strategy.can_wrap_function_in_llm","text":"Return True if f can be wrapped in an LLMCall. Source code in llm_strategy/llm_strategy.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def can_wrap_function_in_llm ( f : typing . Callable [ P , T ]) -> bool : \"\"\" Return True if f can be wrapped in an LLMCall. \"\"\" unwrapped = unwrap_function ( f ) if not callable ( unwrapped ): return False if inspect . isgeneratorfunction ( unwrapped ): return False if inspect . iscoroutinefunction ( unwrapped ): return False if not inspect . isfunction ( unwrapped ) and not inspect . ismethod ( unwrapped ): return False return is_not_implemented ( unwrapped )","title":"can_wrap_function_in_llm()"},{"location":"reference/llm_strategy/llm_strategy/#llm_strategy.llm_strategy.can_wrap_member_in_llm","text":"Return True if f can be wrapped in an LLMCall. Source code in llm_strategy/llm_strategy.py 95 96 97 98 99 100 101 102 103 104 def can_wrap_member_in_llm ( f : typing . Callable [ P , T ]) -> bool : \"\"\" Return True if f can be wrapped in an LLMCall. \"\"\" if isinstance ( f , LLMFunction ): return True if dataclasses . is_dataclass ( f ): return True return can_wrap_function_in_llm ( f )","title":"can_wrap_member_in_llm()"},{"location":"reference/llm_strategy/llm_strategy/#llm_strategy.llm_strategy.llm_strategy","text":"A strategy that implements whatever it decorates (or is called on) using the LLM. Source code in llm_strategy/llm_strategy.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def llm_strategy ( llm : BaseLLM ) -> typing . Callable [[ T ], T ]: # noqa: C901 \"\"\" A strategy that implements whatever it decorates (or is called on) using the LLM. \"\"\" @typing . no_type_check def decorator ( f : T ) -> T : assert can_wrap_member_in_llm ( f ) # For an instance of dataclass, call llm_strategy_dataclass with the fields. if dataclasses . is_dataclass ( f ): if isinstance ( f , type ): return llm_dataclass ( f , llm ) else : implemented_dataclass = llm_dataclass ( type ( f ), llm ) # Create an instance of the implemented dataclass using the fields from f params = { field . name : getattr ( f , field . name ) for field in dataclasses . fields ( f )} return implemented_dataclass ( ** params ) else : def inner_decorator ( unwrapped_f ): llm_f = None @functools . wraps ( unwrapped_f ) def strategy_wrapper ( * args , ** kwargs ): nonlocal llm_f if llm_f is None : # Get the signature of f sig = inspect . signature ( unwrapped_f , eval_str = True ) # Add a llm parameter to the signature as first argument new_params = [ inspect . Parameter ( \"__llm\" , inspect . Parameter . POSITIONAL_ONLY )] new_params . extend ( sig . parameters . values ()) new_sig = sig . replace ( parameters = new_params ) def dummy_f ( * args , ** kwargs ): raise NotImplementedError () new_f = functools . wraps ( unwrapped_f )( dummy_f ) new_f . __module__ = unwrapped_f . __module__ # Set the signature of the new function new_f . __signature__ = new_sig del new_f . __wrapped__ # Wrap the function in an LLMFunction llm_f = functools . wraps ( new_f )( LLMFunction ()) return llm_f ( llm , * args , ** kwargs ) return strategy_wrapper return apply_decorator ( f , inner_decorator ) return decorator","title":"llm_strategy()"},{"location":"reference/llm_strategy/testing/","text":"","title":"testing"},{"location":"reference/llm_strategy/testing/fake_chat_model/","text":"FakeChatModel \u00b6 Bases: BaseChatModel , BaseModel Fake ChatModel wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an external_llm argument, which is an LLM that will be called if the query is not found in the texts dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) Source code in llm_strategy/testing/fake_chat_model.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class FakeChatModel ( BaseChatModel , BaseModel ): \"\"\"Fake ChatModel wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an `external_llm` argument, which is an LLM that will be called if the query is not found in the `texts` dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) \"\"\" messages_tuples_bag : set [ tuple ] = Field ( default_factory = set ) \"\"\"The texts to return on call.\"\"\" external_chat_model : BaseChatModel | None = None \"\"\"An external LLM to use if the query is not found.\"\"\" @property def _llm_type ( self ) -> str : return \"fake\" @staticmethod def from_messages ( messages_bag : Collection [ list [ BaseMessage ]]) -> \"FakeChatModel\" : messages_tuples_bag = { tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for messages in messages_bag } return FakeChatModel ( messages_tuples_bag = messages_tuples_bag ) async def _agenerate ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> ChatResult : raise NotImplementedError def _generate ( self , messages : List [ BaseMessage ], stop : Optional [ List [ str ]] = None , ** kwargs ) -> ChatResult : raise NotImplementedError def __del__ ( self ) -> None : # If we have an external LLM, we write out all our responses to stdout so that they can be copied into the # constructor call for the next run by hand if needed. if self . external_chat_model is not None : # Deduplicate the messages (any shared prefixes can be removed) self . messages_tuples_bag = { messages for messages in self . messages_tuples_bag if not any ( is_prefix_list ( messages , other ) for other in self . messages_tuples_bag if other != messages ) } print ( f \"messages_bag = { self . messages_tuples_bag !r} \" ) def invoke ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> BaseMessage : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" assert stop is None , \"Stop words are not supported for FakeChatModel.\" messages_tuple = tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for cached_messages in self . messages_tuples_bag : if is_prefix_list ( messages_tuple , cached_messages ): # check that the next message is an AIMessage if len ( cached_messages ) == len ( messages_tuple ): raise ValueError ( \"No response found in messages_bag.\" ) next_message = messages_from_dict ([ tuple_to_dict ( cached_messages [ len ( messages )])])[ 0 ] if not isinstance ( next_message , AIMessage ): raise ValueError ( \"No response found in messages_bag.\" ) return next_message if self . external_chat_model is not None : message = self . external_chat_model . invoke ( messages , stop = stop , ** kwargs ) message_tuple = dict_to_tuple ( messages_to_dict ([ message ])[ 0 ]) self . messages_tuples_bag . add ( tuple ( list ( messages_tuple ) + [ message_tuple ])) return message # If no queries are provided, print the code to update the query code_snippet = f \"\"\"# Add the following to the queries dict: { messages !r} , # TODO: Append the correct response here \"\"\" print ( code_snippet ) raise NotImplementedError ( \"No query provided. Add the following to the queries dict: \\n\\n \" + code_snippet ) __call__ = invoke external_chat_model : BaseChatModel | None = None class-attribute instance-attribute \u00b6 An external LLM to use if the query is not found. messages_tuples_bag : set [ tuple ] = Field ( default_factory = set ) class-attribute instance-attribute \u00b6 The texts to return on call. invoke ( messages , stop = None , ** kwargs ) \u00b6 Return the query if it exists, else print the code to update the query. Source code in llm_strategy/testing/fake_chat_model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def invoke ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> BaseMessage : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" assert stop is None , \"Stop words are not supported for FakeChatModel.\" messages_tuple = tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for cached_messages in self . messages_tuples_bag : if is_prefix_list ( messages_tuple , cached_messages ): # check that the next message is an AIMessage if len ( cached_messages ) == len ( messages_tuple ): raise ValueError ( \"No response found in messages_bag.\" ) next_message = messages_from_dict ([ tuple_to_dict ( cached_messages [ len ( messages )])])[ 0 ] if not isinstance ( next_message , AIMessage ): raise ValueError ( \"No response found in messages_bag.\" ) return next_message if self . external_chat_model is not None : message = self . external_chat_model . invoke ( messages , stop = stop , ** kwargs ) message_tuple = dict_to_tuple ( messages_to_dict ([ message ])[ 0 ]) self . messages_tuples_bag . add ( tuple ( list ( messages_tuple ) + [ message_tuple ])) return message # If no queries are provided, print the code to update the query code_snippet = f \"\"\"# Add the following to the queries dict: { messages !r} , # TODO: Append the correct response here \"\"\" print ( code_snippet ) raise NotImplementedError ( \"No query provided. Add the following to the queries dict: \\n\\n \" + code_snippet ) dict_to_tuple ( d ) \u00b6 Convert a dict to a tuple of tuples, sorted by key. Source code in llm_strategy/testing/fake_chat_model.py 30 31 32 33 def dict_to_tuple ( d : Mapping [ Any , Any ]) -> tuple [ tuple [ Any , Any ], ... ]: \"\"\"Convert a dict to a tuple of tuples, sorted by key.\"\"\" # Convert values that are dicts to tuples as well. return tuple ( sorted (( k , dict_to_tuple ( v ) if isinstance ( v , dict ) else v ) for k , v in d . items ())) is_prefix_list ( prefix_candidate , messages ) \u00b6 Return whether prefix_candidate is a prefix of messages . Source code in llm_strategy/testing/fake_chat_model.py 41 42 43 44 45 46 47 48 def is_prefix_list ( prefix_candidate : Collection , messages : Collection ) -> bool : \"\"\"Return whether `prefix_candidate` is a prefix of `messages`.\"\"\" if len ( prefix_candidate ) > len ( messages ): return False for prefix_message , message in zip ( prefix_candidate , messages ): if prefix_message != message : return False return True tuple_to_dict ( t ) \u00b6 Convert a tuple of tuples to a dict. Source code in llm_strategy/testing/fake_chat_model.py 36 37 38 def tuple_to_dict ( t : Iterable [ tuple [ Any , Any ]]) -> dict [ Any , Any ]: \"\"\"Convert a tuple of tuples to a dict.\"\"\" return { k : tuple_to_dict ( v ) if isinstance ( v , tuple ) else v for k , v in t }","title":"fake_chat_model"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.FakeChatModel","text":"Bases: BaseChatModel , BaseModel Fake ChatModel wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an external_llm argument, which is an LLM that will be called if the query is not found in the texts dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) Source code in llm_strategy/testing/fake_chat_model.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class FakeChatModel ( BaseChatModel , BaseModel ): \"\"\"Fake ChatModel wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an `external_llm` argument, which is an LLM that will be called if the query is not found in the `texts` dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) \"\"\" messages_tuples_bag : set [ tuple ] = Field ( default_factory = set ) \"\"\"The texts to return on call.\"\"\" external_chat_model : BaseChatModel | None = None \"\"\"An external LLM to use if the query is not found.\"\"\" @property def _llm_type ( self ) -> str : return \"fake\" @staticmethod def from_messages ( messages_bag : Collection [ list [ BaseMessage ]]) -> \"FakeChatModel\" : messages_tuples_bag = { tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for messages in messages_bag } return FakeChatModel ( messages_tuples_bag = messages_tuples_bag ) async def _agenerate ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> ChatResult : raise NotImplementedError def _generate ( self , messages : List [ BaseMessage ], stop : Optional [ List [ str ]] = None , ** kwargs ) -> ChatResult : raise NotImplementedError def __del__ ( self ) -> None : # If we have an external LLM, we write out all our responses to stdout so that they can be copied into the # constructor call for the next run by hand if needed. if self . external_chat_model is not None : # Deduplicate the messages (any shared prefixes can be removed) self . messages_tuples_bag = { messages for messages in self . messages_tuples_bag if not any ( is_prefix_list ( messages , other ) for other in self . messages_tuples_bag if other != messages ) } print ( f \"messages_bag = { self . messages_tuples_bag !r} \" ) def invoke ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> BaseMessage : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" assert stop is None , \"Stop words are not supported for FakeChatModel.\" messages_tuple = tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for cached_messages in self . messages_tuples_bag : if is_prefix_list ( messages_tuple , cached_messages ): # check that the next message is an AIMessage if len ( cached_messages ) == len ( messages_tuple ): raise ValueError ( \"No response found in messages_bag.\" ) next_message = messages_from_dict ([ tuple_to_dict ( cached_messages [ len ( messages )])])[ 0 ] if not isinstance ( next_message , AIMessage ): raise ValueError ( \"No response found in messages_bag.\" ) return next_message if self . external_chat_model is not None : message = self . external_chat_model . invoke ( messages , stop = stop , ** kwargs ) message_tuple = dict_to_tuple ( messages_to_dict ([ message ])[ 0 ]) self . messages_tuples_bag . add ( tuple ( list ( messages_tuple ) + [ message_tuple ])) return message # If no queries are provided, print the code to update the query code_snippet = f \"\"\"# Add the following to the queries dict: { messages !r} , # TODO: Append the correct response here \"\"\" print ( code_snippet ) raise NotImplementedError ( \"No query provided. Add the following to the queries dict: \\n\\n \" + code_snippet ) __call__ = invoke","title":"FakeChatModel"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.FakeChatModel.external_chat_model","text":"An external LLM to use if the query is not found.","title":"external_chat_model"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.FakeChatModel.messages_tuples_bag","text":"The texts to return on call.","title":"messages_tuples_bag"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.FakeChatModel.invoke","text":"Return the query if it exists, else print the code to update the query. Source code in llm_strategy/testing/fake_chat_model.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def invoke ( self , messages : list [ BaseMessage ], stop : list [ str ] | None = None , ** kwargs ) -> BaseMessage : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" assert stop is None , \"Stop words are not supported for FakeChatModel.\" messages_tuple = tuple ( dict_to_tuple ( m ) for m in messages_to_dict ( messages )) for cached_messages in self . messages_tuples_bag : if is_prefix_list ( messages_tuple , cached_messages ): # check that the next message is an AIMessage if len ( cached_messages ) == len ( messages_tuple ): raise ValueError ( \"No response found in messages_bag.\" ) next_message = messages_from_dict ([ tuple_to_dict ( cached_messages [ len ( messages )])])[ 0 ] if not isinstance ( next_message , AIMessage ): raise ValueError ( \"No response found in messages_bag.\" ) return next_message if self . external_chat_model is not None : message = self . external_chat_model . invoke ( messages , stop = stop , ** kwargs ) message_tuple = dict_to_tuple ( messages_to_dict ([ message ])[ 0 ]) self . messages_tuples_bag . add ( tuple ( list ( messages_tuple ) + [ message_tuple ])) return message # If no queries are provided, print the code to update the query code_snippet = f \"\"\"# Add the following to the queries dict: { messages !r} , # TODO: Append the correct response here \"\"\" print ( code_snippet ) raise NotImplementedError ( \"No query provided. Add the following to the queries dict: \\n\\n \" + code_snippet )","title":"invoke()"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.dict_to_tuple","text":"Convert a dict to a tuple of tuples, sorted by key. Source code in llm_strategy/testing/fake_chat_model.py 30 31 32 33 def dict_to_tuple ( d : Mapping [ Any , Any ]) -> tuple [ tuple [ Any , Any ], ... ]: \"\"\"Convert a dict to a tuple of tuples, sorted by key.\"\"\" # Convert values that are dicts to tuples as well. return tuple ( sorted (( k , dict_to_tuple ( v ) if isinstance ( v , dict ) else v ) for k , v in d . items ()))","title":"dict_to_tuple()"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.is_prefix_list","text":"Return whether prefix_candidate is a prefix of messages . Source code in llm_strategy/testing/fake_chat_model.py 41 42 43 44 45 46 47 48 def is_prefix_list ( prefix_candidate : Collection , messages : Collection ) -> bool : \"\"\"Return whether `prefix_candidate` is a prefix of `messages`.\"\"\" if len ( prefix_candidate ) > len ( messages ): return False for prefix_message , message in zip ( prefix_candidate , messages ): if prefix_message != message : return False return True","title":"is_prefix_list()"},{"location":"reference/llm_strategy/testing/fake_chat_model/#llm_strategy.testing.fake_chat_model.tuple_to_dict","text":"Convert a tuple of tuples to a dict. Source code in llm_strategy/testing/fake_chat_model.py 36 37 38 def tuple_to_dict ( t : Iterable [ tuple [ Any , Any ]]) -> dict [ Any , Any ]: \"\"\"Convert a tuple of tuples to a dict.\"\"\" return { k : tuple_to_dict ( v ) if isinstance ( v , tuple ) else v for k , v in t }","title":"tuple_to_dict()"},{"location":"reference/llm_strategy/testing/fake_llm/","text":"FakeLLM \u00b6 Bases: LLM , BaseModel Fake LLM wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an external_llm argument, which is an LLM that will be called if the query is not found in the texts dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) Source code in llm_strategy/testing/fake_llm.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class FakeLLM ( LLM , BaseModel ): \"\"\"Fake LLM wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an `external_llm` argument, which is an LLM that will be called if the query is not found in the `texts` dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) \"\"\" texts : set [ str ] = Field ( default_factory = set ) \"\"\"The texts to return on call.\"\"\" external_llm : BaseLLM | None = None \"\"\"An external LLM to use if the query is not found.\"\"\" def __del__ ( self ) -> None : # If we have an external LLM, we write out all our responses to stdout so that they can be copied into the # constructor call for the next run by hand if needed. if self . external_llm is not None : # Deduplicate the texts (any shared prefixes can be removed) self . texts = { text for text in self . texts if not any ( other . startswith ( text ) for other in self . texts if other != text ) } print ( f \"texts = { self . texts !r} \" ) @property def _llm_type ( self ) -> str : \"\"\"Return type of llm.\"\"\" return \"fake\" def _call ( self , prompt : str , stop : list [ str ] | None = None ) -> str : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" for text in self . texts : if text . startswith ( prompt ): # Remainder: response = text [ len ( prompt ) :] # Emulate stop behavior if stop is not None : for stop_word in stop : if stop_word in response : # Only return the answer up to the stop word response = response [: response . index ( stop_word )] return response if self . external_llm is not None : response = self . external_llm . invoke ( prompt , stop = stop ) text = prompt + response self . texts . add ( text ) return response # If no queries are provided, print the code to update the query code_snippet = ( f \"# Add the following to the queries list: \\n\\n { repr ( prompt ) } \\n # TODO: Append the correct response here\" ) print ( code_snippet ) raise NotImplementedError ( \"No query provided to FakeLLM.\" + code_snippet ) @property def _identifying_params ( self ) -> Mapping [ str , Any ]: return {} external_llm : BaseLLM | None = None class-attribute instance-attribute \u00b6 An external LLM to use if the query is not found. texts : set [ str ] = Field ( default_factory = set ) class-attribute instance-attribute \u00b6 The texts to return on call.","title":"fake_llm"},{"location":"reference/llm_strategy/testing/fake_llm/#llm_strategy.testing.fake_llm.FakeLLM","text":"Bases: LLM , BaseModel Fake LLM wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an external_llm argument, which is an LLM that will be called if the query is not found in the texts dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) Source code in llm_strategy/testing/fake_llm.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class FakeLLM ( LLM , BaseModel ): \"\"\"Fake LLM wrapper for testing purposes. We can use this to test the behavior of the LLM wrapper without having to actually call the LLM. We support an `external_llm` argument, which is an LLM that will be called if the query is not found in the `texts` dict. We store the responses. On exit, we deduplicate them and print them to stdout so that they can be copied into constructor call for the next run by hand if needed. We support stop words, which are words that are removed from the response if they are found. To do so, we store the full response (as it is build over time) and return the part before the query and the stop word. This also means that there is no non-determinism in the output, which is good for testing, but bad for variance. Especially if we want to test the behavior of the LLM wrapper when the LLM is not deterministic. (Create different outputs for different calls, for example.) \"\"\" texts : set [ str ] = Field ( default_factory = set ) \"\"\"The texts to return on call.\"\"\" external_llm : BaseLLM | None = None \"\"\"An external LLM to use if the query is not found.\"\"\" def __del__ ( self ) -> None : # If we have an external LLM, we write out all our responses to stdout so that they can be copied into the # constructor call for the next run by hand if needed. if self . external_llm is not None : # Deduplicate the texts (any shared prefixes can be removed) self . texts = { text for text in self . texts if not any ( other . startswith ( text ) for other in self . texts if other != text ) } print ( f \"texts = { self . texts !r} \" ) @property def _llm_type ( self ) -> str : \"\"\"Return type of llm.\"\"\" return \"fake\" def _call ( self , prompt : str , stop : list [ str ] | None = None ) -> str : \"\"\"Return the query if it exists, else print the code to update the query.\"\"\" for text in self . texts : if text . startswith ( prompt ): # Remainder: response = text [ len ( prompt ) :] # Emulate stop behavior if stop is not None : for stop_word in stop : if stop_word in response : # Only return the answer up to the stop word response = response [: response . index ( stop_word )] return response if self . external_llm is not None : response = self . external_llm . invoke ( prompt , stop = stop ) text = prompt + response self . texts . add ( text ) return response # If no queries are provided, print the code to update the query code_snippet = ( f \"# Add the following to the queries list: \\n\\n { repr ( prompt ) } \\n # TODO: Append the correct response here\" ) print ( code_snippet ) raise NotImplementedError ( \"No query provided to FakeLLM.\" + code_snippet ) @property def _identifying_params ( self ) -> Mapping [ str , Any ]: return {}","title":"FakeLLM"},{"location":"reference/llm_strategy/testing/fake_llm/#llm_strategy.testing.fake_llm.FakeLLM.external_llm","text":"An external LLM to use if the query is not found.","title":"external_llm"},{"location":"reference/llm_strategy/testing/fake_llm/#llm_strategy.testing.fake_llm.FakeLLM.texts","text":"The texts to return on call.","title":"texts"}]}